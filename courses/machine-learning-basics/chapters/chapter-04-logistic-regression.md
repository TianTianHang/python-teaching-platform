---
title: "é€»è¾‘å›å½’"
order: 4
unlock_conditions:
  type: "prerequisite"
  prerequisites: [3]
---
## é€»è¾‘å›å½’

### ç« èŠ‚æ¦‚è¿°

é€»è¾‘å›å½’æ˜¯åˆ†ç±»é—®é¢˜çš„æ ¸å¿ƒç®—æ³•ï¼Œè™½ç„¶åå­—ä¸­æœ‰"å›å½’"ï¼Œä½†å®é™…ä¸Šæ˜¯ä¸€ç§åˆ†ç±»ç®—æ³•ã€‚å®ƒé€šè¿‡ Sigmoid å‡½æ•°å°†çº¿æ€§è¾“å‡ºæ˜ å°„åˆ° [0, 1] åŒºé—´ï¼Œè¡¨ç¤ºæ ·æœ¬å±äºæŸä¸€ç±»åˆ«çš„æ¦‚ç‡ã€‚æœ¬ç« å°†å­¦ä¹ äºŒåˆ†ç±»é—®é¢˜çš„è§£å†³æ–¹æ³•ã€‚

### çŸ¥è¯†ç‚¹ 1ï¼šåˆ†ç±»é—®é¢˜ä¸ Sigmoid å‡½æ•°

**æè¿°ï¼š**

åˆ†ç±»é—®é¢˜è¦æ±‚é¢„æµ‹ç¦»æ•£çš„ç±»åˆ«æ ‡ç­¾ï¼Œè€Œé€»è¾‘å›å½’é€šè¿‡å¼•å…¥ Sigmoid å‡½æ•°ï¼Œå°†çº¿æ€§å›å½’çš„è¿ç»­è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡å€¼ã€‚

**Sigmoid å‡½æ•°ï¼š**

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**æ€§è´¨ï¼š**
- è¾“å‡ºèŒƒå›´ï¼š(0, 1)
- $\sigma(0) = 0.5$ï¼Œæ˜¯å†³ç­–è¾¹ç•Œé˜ˆå€¼
- å½“ $z \to \infty$ æ—¶ï¼Œ$\sigma(z) \to 1$
- å½“ $z \to -\infty$ æ—¶ï¼Œ$\sigma(z) \to 0$

**ç¤ºä¾‹ä»£ç ï¼š**

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    """
    Sigmoid æ¿€æ´»å‡½æ•°

    Args:
        z: çº¿æ€§ç»„åˆç»“æœ

    Returns:
        æ¦‚ç‡å€¼ï¼ŒèŒƒå›´ (0, 1)
    """
    return 1 / (1 + np.exp(-z))

# å¯è§†åŒ– Sigmoid å‡½æ•°
z = np.linspace(-10, 10, 100)
sigma = sigmoid(z)

plt.figure(figsize=(10, 4))

# Sigmoid å‡½æ•°æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(z, sigma, 'b-', linewidth=2)
plt.axhline(y=0.5, color='r', linestyle='--', label='é˜ˆå€¼ 0.5')
plt.axvline(x=0, color='g', linestyle='--', label='z=0')
plt.xlabel('z')
plt.ylabel('sigmoid(z)')
plt.title('Sigmoid å‡½æ•°')
plt.legend()
plt.grid(True, linestyle=':', alpha=0.5)

# å†³ç­–è¾¹ç•Œç¤ºæ„å›¾
plt.subplot(1, 2, 2)
plt.axhline(y=0.5, color='r', linestyle='-', linewidth=2)
plt.fill_between([-10, 0], 0, 0.5, alpha=0.3, color='blue', label='é¢„æµ‹ç±»åˆ« 0')
plt.fill_between([0, 10], 0.5, 1, alpha=0.3, color='orange', label='é¢„æµ‹ç±»åˆ« 1')
plt.xlim(-10, 10)
plt.ylim(0, 1)
plt.xlabel('z')
plt.ylabel('æ¦‚ç‡')
plt.title('å†³ç­–è¾¹ç•Œ')
plt.legend()
plt.grid(True, linestyle=':', alpha=0.5)

plt.tight_layout()
plt.show()

# æµ‹è¯• Sigmoid å‡½æ•°
test_values = [-10, -5, -1, 0, 1, 5, 10]
print("Sigmoid å‡½æ•°å€¼æµ‹è¯•:")
for val in test_values:
    print(f"sigmoid({val:3d}) = {sigmoid(val):.6f}")
```

**è§£é‡Šï¼š**

- Sigmoid å‡½æ•°å°†ä»»æ„å®æ•°æ˜ å°„åˆ° (0, 1) åŒºé—´
- è¾“å‡ºå¯ä»¥è§£é‡Šä¸ºæ ·æœ¬å±äºæ­£ç±»çš„æ¦‚ç‡
- å½“æ¦‚ç‡ â‰¥ 0.5 æ—¶ï¼Œé¢„æµ‹ä¸ºç±»åˆ« 1ï¼›å¦åˆ™é¢„æµ‹ä¸ºç±»åˆ« 0
- å†³ç­–è¾¹ç•Œä½äº $z = 0$ å¤„ï¼Œå³ $w \cdot x + b = 0$

### çŸ¥è¯†ç‚¹ 2ï¼šé€»è¾‘å›å½’æŸå¤±å‡½æ•°

**æè¿°ï¼š**

çº¿æ€§å›å½’ä½¿ç”¨å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œä½†å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œå‡æ–¹è¯¯å·®æ˜¯éå‡¸çš„ï¼Œä¼šå¯¼è‡´æ¢¯åº¦ä¸‹é™é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚é€»è¾‘å›å½’ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå®ƒæ˜¯å‡¸å‡½æ•°ï¼Œå¯ä»¥ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚

**äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼š**

$$J(w,b) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(\hat{y}^{(i)}) + (1-y^{(i)})\log(1-\hat{y}^{(i)})]$$

å…¶ä¸­ $\hat{y} = \sigma(w \cdot x + b)$ æ˜¯æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡ã€‚

**ç›´è§‚ç†è§£ï¼š**

- å½“ $y = 1$ æ—¶ï¼ŒæŸå¤± $= -\log(\hat{y})$ï¼Œé¢„æµ‹è¶Šæ¥è¿‘ 1ï¼ŒæŸå¤±è¶Šå°
- å½“ $y = 0$ æ—¶ï¼ŒæŸå¤± $= -\log(1-\hat{y})$ï¼Œé¢„æµ‹è¶Šæ¥è¿‘ 0ï¼ŒæŸå¤±è¶Šå°

**ç¤ºä¾‹ä»£ç ï¼š**

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_loss(X, y, w, b):
    """
    è®¡ç®—äº¤å‰ç†µæŸå¤±

    Args:
        X: ç‰¹å¾æ•°æ®ï¼Œå½¢çŠ¶ (m, n)
        y: æ ‡ç­¾æ•°æ®ï¼Œå½¢çŠ¶ (m,)ï¼Œå€¼ä¸º 0 æˆ– 1
        w: æƒé‡ï¼Œå½¢çŠ¶ (n,)
        b: åç½®

    Returns:
        æŸå¤±å€¼
    """
    m = X.shape[0]

    # å‰å‘ä¼ æ’­
    z = np.dot(X, w) + b
    y_pred = sigmoid(z)

    # è®¡ç®—äº¤å‰ç†µæŸå¤±
    # æ·»åŠ å°å€¼é¿å… log(0)
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)

    loss = -(1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))

    return loss

# æµ‹è¯•æŸå¤±å‡½æ•°
# ç®€å•ç¤ºä¾‹ï¼šæ­£ç¡®é¢„æµ‹æ—¶æŸå¤±å°ï¼Œé”™è¯¯é¢„æµ‹æ—¶æŸå¤±å¤§
print("æŸå¤±å‡½æ•°æµ‹è¯•:")
print(f"é¢„æµ‹ 0.9ï¼ŒçœŸå® 1: æŸå¤± = {-np.log(0.9):.4f}")
print(f"é¢„æµ‹ 0.1ï¼ŒçœŸå® 0: æŸå¤± = {-np.log(0.9):.4f}")
print(f"é¢„æµ‹ 0.1ï¼ŒçœŸå® 1: æŸå¤± = {-np.log(0.1):.4f}")
print(f"é¢„æµ‹ 0.9ï¼ŒçœŸå® 0: æŸå¤± = {-np.log(0.1):.4f}")
```

**æ¢¯åº¦è®¡ç®—ï¼š**

$$\frac{\partial J}{\partial w} = \frac{1}{m}X^T(\hat{y} - y)$$
$$\frac{\partial J}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(\hat{y}^{(i)} - y^{(i)})$$

### çŸ¥è¯†ç‚¹ 3ï¼šscikit-learn é€»è¾‘å›å½’

**æè¿°ï¼š**

scikit-learn æä¾›äº†é«˜æ•ˆçš„é€»è¾‘å›å½’å®ç°ï¼Œæ”¯æŒå¤šç§ä¼˜åŒ–ç®—æ³•å’Œæ­£åˆ™åŒ–æ–¹æ³•ã€‚

**ç¤ºä¾‹ä»£ç ï¼š**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import make_classification

# åˆ›å»ºäºŒåˆ†ç±»æ•°æ®é›†
X, y = make_classification(
    n_samples=200,
    n_features=2,
    n_redundant=0,
    n_informative=2,
    random_state=42,
    n_clusters_per_class=1
)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# æŸ¥çœ‹æ¨¡å‹å‚æ•°
print(f"æƒé‡ (coef_): {model.coef_}")
print(f"åç½® (intercept_): {model.intercept_}")

# è¿›è¡Œé¢„æµ‹
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)

# è¾“å‡ºé¢„æµ‹æ¦‚ç‡ç¤ºä¾‹
print("\né¢„æµ‹æ¦‚ç‡ç¤ºä¾‹ï¼ˆå‰5ä¸ªæ ·æœ¬ï¼‰:")
print("ç±»åˆ« 0 çš„æ¦‚ç‡ | ç±»åˆ« 1 çš„æ¦‚ç‡ | é¢„æµ‹ç±»åˆ« | çœŸå®ç±»åˆ«")
for i in range(5):
    print(f"    {y_pred_proba[i, 0]:.4f}    |    {y_pred_proba[i, 1]:.4f}    |    {y_pred[i]}       |    {y_test[i]}")

# è¯„ä¼°æ¨¡å‹
accuracy = accuracy_score(y_test, y_pred)
print(f"\nå‡†ç¡®ç‡: {accuracy:.4f}")

print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))

print("æ··æ·†çŸ©é˜µ:")
print(confusion_matrix(y_test, y_pred))

# å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
plt.figure(figsize=(12, 5))

# è®­ç»ƒæ•°æ®
plt.subplot(1, 2, 1)
plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],
            c='blue', marker='o', label='ç±»åˆ« 0', alpha=0.6)
plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],
            c='red', marker='s', label='ç±»åˆ« 1', alpha=0.6)

# ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z = Z.reshape(xx.shape)
plt.contour(xx, yy, Z, levels=[0.5], colors='green', linestyles='--', linewidths=2)
plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.2, colors=['blue', 'red'])
plt.xlabel('ç‰¹å¾ 1')
plt.ylabel('ç‰¹å¾ 2')
plt.title('è®­ç»ƒæ•°æ®å’Œå†³ç­–è¾¹ç•Œ')
plt.legend()
plt.grid(True, linestyle=':', alpha=0.5)

# æµ‹è¯•æ•°æ®é¢„æµ‹ç»“æœ
plt.subplot(1, 2, 2)
# æ­£ç¡®é¢„æµ‹
correct = y_pred == y_test
plt.scatter(X_test[correct & (y_test == 0), 0], X_test[correct & (y_test == 0), 1],
            c='blue', marker='o', label='æ­£ç¡®é¢„æµ‹ï¼ˆç±»åˆ« 0ï¼‰', alpha=0.6)
plt.scatter(X_test[correct & (y_test == 1), 0], X_test[correct & (y_test == 1), 1],
            c='red', marker='s', label='æ­£ç¡®é¢„æµ‹ï¼ˆç±»åˆ« 1ï¼‰', alpha=0.6)
# é”™è¯¯é¢„æµ‹
wrong = ~correct
plt.scatter(X_test[wrong & (y_test == 0), 0], X_test[wrong & (y_test == 0), 1],
            c='blue', marker='x', s=100, label='é”™è¯¯é¢„æµ‹ï¼ˆå®é™…ä¸º 0ï¼‰', linewidths=2)
plt.scatter(X_test[wrong & (y_test == 1), 0], X_test[wrong & (y_test == 1), 1],
            c='red', marker='x', s=100, label='é”™è¯¯é¢„æµ‹ï¼ˆå®é™…ä¸º 1ï¼‰', linewidths=2)

# ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
plt.contour(xx, yy, Z, levels=[0.5], colors='green', linestyles='--', linewidths=2)
plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.2, colors=['blue', 'red'])
plt.xlabel('ç‰¹å¾ 1')
plt.ylabel('ç‰¹å¾ 2')
plt.title('æµ‹è¯•æ•°æ®é¢„æµ‹ç»“æœ')
plt.legend()
plt.grid(True, linestyle=':', alpha=0.5)

plt.tight_layout()
plt.show()
```

**è§£é‡Šï¼š**

- `predict_proba()`ï¼šè¿”å›æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œå½¢çŠ¶ (n_samples, n_classes)
- `predict()`ï¼šè¿”å›é¢„æµ‹çš„ç±»åˆ«æ ‡ç­¾
- `coef_`ï¼šæƒé‡å‚æ•°ï¼Œè¡¨ç¤ºæ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§
- `intercept_`ï¼šåç½®å‚æ•°
- å†³ç­–è¾¹ç•Œæ˜¯æ¦‚ç‡ç­‰äº 0.5 çš„çº¿

### çŸ¥è¯†ç‚¹ 4ï¼šåˆ†ç±»è¯„ä¼°æŒ‡æ ‡

**æè¿°ï¼š**

å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œå‡†ç¡®ç‡ä¸æ˜¯å”¯ä¸€çš„è¯„ä¼°æŒ‡æ ‡ã€‚å½“æ•°æ®ä¸å¹³è¡¡æ—¶ï¼Œè¿˜éœ€è¦å…³æ³¨ç²¾ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚

**è¯„ä¼°æŒ‡æ ‡è¯¦è§£ï¼š**

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix, classification_report
)

# æ··æ·†çŸ©é˜µ
#               é¢„æµ‹ç±»åˆ« 0   é¢„æµ‹ç±»åˆ« 1
# å®é™…ç±»åˆ« 0      TN            FP
# å®é™…ç±»åˆ« 1      FN            TP
#
# TN: True Negative  æ­£ç¡®é¢„æµ‹ä¸ºè´Ÿä¾‹
# FP: False Positive é”™è¯¯é¢„æµ‹ä¸ºæ­£ä¾‹ï¼ˆç¬¬ä¸€ç±»é”™è¯¯ï¼‰
# FN: False Negative é”™è¯¯é¢„æµ‹ä¸ºè´Ÿä¾‹ï¼ˆç¬¬äºŒç±»é”™è¯¯ï¼‰
# TP: True Positive  æ­£ç¡®é¢„æµ‹ä¸ºæ­£ä¾‹

# å‡è®¾æœ‰é¢„æµ‹ç»“æœ
y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
y_pred = np.array([0, 1, 0, 0, 0, 1, 1, 1, 0, 1])

# è®¡ç®—å„é¡¹æŒ‡æ ‡
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"å‡†ç¡®ç‡ (Accuracy):  {accuracy:.4f}")
print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
print(f"å¬å›ç‡ (Recall):    {recall:.4f}")
print(f"F1 åˆ†æ•°:           {f1:.4f}")

# è®¡ç®—å…¬å¼
print("\nè®¡ç®—å…¬å¼:")
print("å‡†ç¡®ç‡  = (TP + TN) / (TP + TN + FP + FN)  - æ‰€æœ‰é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹")
print("ç²¾ç¡®ç‡ = TP / (TP + FP)                     - é¢„æµ‹ä¸ºæ­£ä¾‹ä¸­çœŸæ­£ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹")
print("å¬å›ç‡ = TP / (TP + FN)                     - çœŸæ­£æ­£ä¾‹ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹")
print("F1 åˆ†æ•° = 2 * ç²¾ç¡®ç‡ * å¬å›ç‡ / (ç²¾ç¡®ç‡ + å¬å›ç‡)")

# æ··æ·†çŸ©é˜µå¯è§†åŒ–
cm = confusion_matrix(y_true, y_pred)
print("\næ··æ·†çŸ©é˜µ:")
print(cm)

# åˆ†ç±»æŠ¥å‘Šï¼ˆåŒ…å«æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡ï¼‰
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_true, y_pred))
```

**æŒ‡æ ‡é€‰æ‹©åœºæ™¯ï¼š**

| åœºæ™¯ | é‡è¦æŒ‡æ ‡ | åŸå›  |
|------|----------|------|
| åƒåœ¾é‚®ä»¶åˆ†ç±» | ç²¾ç¡®ç‡ | ä¸å¸Œæœ›è¯¯åˆ¤æ­£å¸¸é‚®ä»¶ä¸ºåƒåœ¾é‚®ä»¶ |
| ç–¾ç—…ç­›æŸ¥ | å¬å›ç‡ | ä¸å¸Œæœ›æ¼æ‰çœŸæ­£çš„ç—…äºº |
| ä¸€èˆ¬åˆ†ç±» | F1 åˆ†æ•° | å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡ |
| å¹³è¡¡æ•°æ®é›† | å‡†ç¡®ç‡ | å„ç±»åˆ«æ•°é‡ç›¸è¿‘ |

### æœ¬ç« å°ç»“

æœ¬ç« å­¦ä¹ äº†é€»è¾‘å›å½’è¿™ä¸€æ ¸å¿ƒåˆ†ç±»ç®—æ³•ï¼š

- **Sigmoid å‡½æ•°**ï¼šå°†çº¿æ€§è¾“å‡ºæ˜ å°„ä¸ºæ¦‚ç‡
- **äº¤å‰ç†µæŸå¤±**ï¼šé€‚åˆåˆ†ç±»é—®é¢˜çš„å‡¸æŸå¤±å‡½æ•°
- **sklearn å®ç°**ï¼šä½¿ç”¨ LogisticRegression è¿›è¡ŒäºŒåˆ†ç±»
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1 åˆ†æ•°

é€»è¾‘å›å½’æ˜¯åˆ†ç±»ç®—æ³•çš„èµ·ç‚¹ï¼Œè™½ç„¶ç®€å•ï¼Œä½†åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­æ•ˆæœè‰¯å¥½ï¼Œå°¤å…¶é€‚åˆä½œä¸ºåŸºçº¿æ¨¡å‹ã€‚

### ğŸ““ ç¬”è®°æœ¬ç»ƒä¹ 

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œè¯·é€šè¿‡ä»¥ä¸‹ç¬”è®°æœ¬ç»ƒä¹ æ¥å·©å›ºä½ çš„çŸ¥è¯†ï¼š

- **[ç»ƒä¹ ç¬”è®°æœ¬](https://tiantianhang.github.io/python-teaching-platform/notebooks/index.html?fromURL=https://raw.githubusercontent.com/TianTianHang/python-teaching-platform/refs/heads/course-content-v1/courses/machine-learning-basics/notebooks/chapter-04-logistic-regression-lab.ipynb)** - é€»è¾‘å›å½’å®è·µç»ƒä¹ 

> ğŸ’¡ **æç¤º**ï¼šå®Œæˆç»ƒä¹ åå¯ä»¥å‚è€ƒç­”æ¡ˆç¬”è®°æœ¬æ¥æ£€æŸ¥ä½ çš„ç­”æ¡ˆã€‚

- **[ç­”æ¡ˆç¬”è®°æœ¬](https://tiantianhang.github.io/python-teaching-platform/notebooks/index.html?fromURL=https://raw.githubusercontent.com/TianTianHang/python-teaching-platform/refs/heads/course-content-v1/courses/machine-learning-basics/solutions/chapter-04-logistic-regression-lab-solution.ipynb)** - ç»ƒä¹ å‚è€ƒç­”æ¡ˆ

---
*æœ¬ç« å†…å®¹åŸºäº Python æ•™å­¦å¹³å°æ ‡å‡†æ ¼å¼è®¾è®¡ã€‚*
