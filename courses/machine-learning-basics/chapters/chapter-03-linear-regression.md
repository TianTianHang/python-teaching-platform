---
title: "çº¿æ€§å›å½’"
order: 3
unlock_conditions:
  type: "prerequisite"
  prerequisites: [2]
---
## çº¿æ€§å›å½’

### ç« èŠ‚æ¦‚è¿°

çº¿æ€§å›å½’æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€åŸºç¡€ä¹Ÿæ˜¯æœ€é‡è¦çš„ç®—æ³•ä¹‹ä¸€ã€‚å®ƒé€šè¿‡æ‹Ÿåˆä¸€æ¡ç›´çº¿ï¼ˆæˆ–è¶…å¹³é¢ï¼‰æ¥é¢„æµ‹è¿ç»­å€¼ï¼Œæ˜¯è®¸å¤šå¤æ‚ç®—æ³•çš„åŸºç¡€ã€‚æœ¬ç« å°†ä»æ•°å­¦åŸç†å‡ºå‘ï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨ NumPy æ‰‹åŠ¨å®ç°çº¿æ€§å›å½’ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ scikit-learn å¿«é€Ÿæ„å»ºæ¨¡å‹ã€‚

### çŸ¥è¯†ç‚¹ 1ï¼šçº¿æ€§å›å½’æ¦‚å¿µ

**æè¿°ï¼š**

çº¿æ€§å›å½’æ˜¯ä¸€ç§ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œç”¨äºé¢„æµ‹è¿ç»­å€¼ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯æ‰¾åˆ°ä¸€ç»„å‚æ•°ï¼Œä½¿å¾—æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®æœ€å°ã€‚

**æ•°å­¦å…¬å¼ï¼š**

- ç®€å•çº¿æ€§å›å½’ï¼ˆä¸€ä¸ªç‰¹å¾ï¼‰ï¼š$h(x) = w \cdot x + b$
  - $w$ï¼šæƒé‡ï¼ˆæ–œç‡ï¼‰
  - $b$ï¼šåç½®ï¼ˆæˆªè·ï¼‰

- å¤šå…ƒçº¿æ€§å›å½’ï¼ˆå¤šä¸ªç‰¹å¾ï¼‰ï¼š$h(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b$

**æŸå¤±å‡½æ•°ï¼ˆå‡æ–¹è¯¯å·® MSEï¼‰ï¼š**

$$J(w,b) = \frac{1}{m}\sum_{i=1}^{m}(h(x^{(i)}) - y^{(i)})^2$$

**è§£é‡Šï¼š**

- **å‡è®¾å‡½æ•° $h(x)$**ï¼šæ¨¡å‹çš„é¢„æµ‹å…¬å¼
- **æŸå¤±å‡½æ•°**ï¼šè¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®è·
- **ç›®æ ‡**ï¼šæ‰¾åˆ°ä½¿æŸå¤±å‡½æ•°æœ€å°çš„ $w$ å’Œ $b$

### çŸ¥è¯†ç‚¹ 2ï¼šæ¢¯åº¦ä¸‹é™ç®—æ³•

**æè¿°ï¼š**

æ¢¯åº¦ä¸‹é™æ˜¯ä¼˜åŒ–æŸå¤±å‡½æ•°çš„å¸¸ç”¨æ–¹æ³•ã€‚å®ƒé€šè¿‡æ²¿æ¢¯åº¦è´Ÿæ–¹å‘è¿­ä»£æ›´æ–°å‚æ•°ï¼Œé€æ­¥æ‰¾åˆ°æœ€å°å€¼ã€‚

**æ¢¯åº¦æ›´æ–°å…¬å¼ï¼š**

$$w := w - \alpha \cdot \frac{\partial J}{\partial w}$$
$$b := b - \alpha \cdot \frac{\partial J}{\partial b}$$

å…¶ä¸­ $\alpha$ æ˜¯å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰ï¼Œæ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•¿ã€‚

**ç¤ºä¾‹ä»£ç ï¼ˆNumPy å®ç°ï¼‰ï¼š**

```python
import numpy as np

def linear_regression_gd(X, y, learning_rate=0.01, epochs=1000):
    """
    ä½¿ç”¨æ¢¯åº¦ä¸‹é™å®ç°çº¿æ€§å›å½’

    Args:
        X: ç‰¹å¾æ•°æ®ï¼Œå½¢çŠ¶ (m, n)ï¼Œm ä¸ºæ ·æœ¬æ•°ï¼Œn ä¸ºç‰¹å¾æ•°
        y: æ ‡ç­¾æ•°æ®ï¼Œå½¢çŠ¶ (m,)
        learning_rate: å­¦ä¹ ç‡
        epochs: è¿­ä»£æ¬¡æ•°

    Returns:
        w: æƒé‡å‚æ•°ï¼Œå½¢çŠ¶ (n,)
        b: åç½®å‚æ•°
        costs: æ¯æ¬¡è¿­ä»£çš„æŸå¤±å€¼
    """
    m, n = X.shape

    # åˆå§‹åŒ–å‚æ•°
    w = np.zeros(n)
    b = 0

    # è®°å½•æŸå¤±
    costs = []

    for epoch in range(epochs):
        # å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹å€¼
        y_pred = np.dot(X, w) + b

        # è®¡ç®—æŸå¤±
        cost = (1 / (2 * m)) * np.sum((y_pred - y) ** 2)
        costs.append(cost)

        # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        dw = (1 / m) * np.dot(X.T, (y_pred - y))
        db = (1 / m) * np.sum(y_pred - y)

        # æ›´æ–°å‚æ•°
        w = w - learning_rate * dw
        b = b - learning_rate * db

        # æ¯ 100 æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡æŸå¤±
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Cost: {cost:.4f}")

    return w, b, costs

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»ºç®€å•çº¿æ€§æ•°æ®
    np.random.seed(42)
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X + np.random.randn(100, 1)

    # è®­ç»ƒæ¨¡å‹
    w, b, costs = linear_regression_gd(X, y, learning_rate=0.1, epochs=1000)

    print(f"\nè®­ç»ƒç»“æœ:")
    print(f"æƒé‡ w: {w[0]:.4f} (çœŸå®å€¼: 3)")
    print(f"åç½® b: {b:.4f} (çœŸå®å€¼: 4)")

    # é¢„æµ‹
    X_new = np.array([[0], [2]])
    y_pred = np.dot(X_new, w) + b
    print(f"\né¢„æµ‹ç»“æœ:")
    print(f"X=0 æ—¶é¢„æµ‹: {y_pred[0]:.4f}")
    print(f"X=2 æ—¶é¢„æµ‹: {y_pred[1]:.4f}")
```

**è§£é‡Šï¼š**

- **å‰å‘ä¼ æ’­**ï¼šè®¡ç®—æ¨¡å‹é¢„æµ‹å€¼
- **è®¡ç®—æŸå¤±**ï¼šä½¿ç”¨å‡æ–¹è¯¯å·®è¡¡é‡è¯¯å·®
- **åå‘ä¼ æ’­**ï¼šè®¡ç®—æŸå¤±å¯¹å‚æ•°çš„æ¢¯åº¦
- **å‚æ•°æ›´æ–°**ï¼šæ²¿æ¢¯åº¦è´Ÿæ–¹å‘æ›´æ–°å‚æ•°
- **å­¦ä¹ ç‡é€‰æ‹©**ï¼šå¤ªå¤§å¯èƒ½éœ‡è¡ï¼Œå¤ªå°æ”¶æ•›æ…¢

### çŸ¥è¯†ç‚¹ 3ï¼šå¤šå…ƒçº¿æ€§å›å½’ä¸ç‰¹å¾ç¼©æ”¾

**æè¿°ï¼š**

å½“æœ‰å¤šä¸ªç‰¹å¾æ—¶ï¼Œä¸åŒç‰¹å¾çš„å°ºåº¦å¯èƒ½å·®å¼‚å¾ˆå¤§ï¼Œè¿™ä¼šå½±å“æ¢¯åº¦ä¸‹é™çš„æ”¶æ•›é€Ÿåº¦ã€‚ç‰¹å¾ç¼©æ”¾å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

**ç¤ºä¾‹ä»£ç ï¼š**

```python
import numpy as np

def normalize(X):
    """
    æ ‡å‡†åŒ–ç‰¹å¾ï¼šä½¿æ¯ä¸ªç‰¹å¾çš„å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1

    Args:
        X: åŸå§‹ç‰¹å¾ï¼Œå½¢çŠ¶ (m, n)

    Returns:
        X_normalized: æ ‡å‡†åŒ–åçš„ç‰¹å¾
        mean: æ¯åˆ—çš„å‡å€¼
        std: æ¯åˆ—çš„æ ‡å‡†å·®
    """
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    X_normalized = (X - mean) / std
    return X_normalized, mean, std

# å¤šå…ƒçº¿æ€§å›å½’ç¤ºä¾‹
if __name__ == "__main__":
    np.random.seed(42)

    # åˆ›å»ºå¤šå…ƒçº¿æ€§æ•°æ®
    # y = 5 + 2*x1 + 3*x2 + 1*x3 + noise
    m = 100
    X1 = np.random.randn(m, 1) * 10  # å¤§å°ºåº¦ç‰¹å¾
    X2 = np.random.randn(m, 1) * 0.1  # å°å°ºåº¦ç‰¹å¾
    X3 = np.random.randn(m, 1)
    X = np.hstack([X1, X2, X3])

    true_w = np.array([2, 3, 1])
    true_b = 5
    y = true_b + np.dot(X, true_w) + np.random.randn(m, 1) * 0.5

    # ä¸ä½¿ç”¨ç‰¹å¾ç¼©æ”¾
    print("ä¸ä½¿ç”¨ç‰¹å¾ç¼©æ”¾:")
    w1, b1, costs1 = linear_regression_gd(X, y, learning_rate=0.0001, epochs=1000)
    print(f"æƒé‡: {w1}")
    print(f"åç½®: {b1:.4f}")
    print(f"æœ€ç»ˆæŸå¤±: {costs1[-1]:.4f}\n")

    # ä½¿ç”¨ç‰¹å¾ç¼©æ”¾
    print("ä½¿ç”¨ç‰¹å¾ç¼©æ”¾:")
    X_normalized, mean, std = normalize(X)
    w2, b2, costs2 = linear_regression_gd(X_normalized, y, learning_rate=0.1, epochs=1000)
    print(f"æƒé‡: {w2}")
    print(f"åç½®: {b2:.4f}")
    print(f"æœ€ç»ˆæŸå¤±: {costs2[-1]:.4f}")
```

**è§£é‡Šï¼š**

- **ç‰¹å¾ç¼©æ”¾çš„å¿…è¦æ€§**ï¼šä¸åŒå°ºåº¦ç‰¹å¾å¯¼è‡´æ¢¯åº¦ä¸‹é™åœ¨"çª„è°·"ä¸­ç¼“æ…¢å‰è¡Œ
- **æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰**ï¼š$x' = \frac{x - \mu}{\sigma}$
- **å½’ä¸€åŒ–ï¼ˆMin-Maxï¼‰**ï¼š$x' = \frac{x - min}{max - min}$
- æ ‡å‡†åŒ–åçš„ç‰¹å¾å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1

### çŸ¥è¯†ç‚¹ 4ï¼šscikit-learn çº¿æ€§å›å½’

**æè¿°ï¼š**

scikit-learn æä¾›äº†ç®€æ´çš„ APIï¼Œä½¿å¾—ä½¿ç”¨çº¿æ€§å›å½’å˜å¾—éå¸¸ç®€å•ã€‚å®ƒå†…éƒ¨ä½¿ç”¨äº†æ›´é«˜æ•ˆçš„ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚æœ€å°äºŒä¹˜æ³•çš„è§£æè§£ï¼‰ã€‚

**ç¤ºä¾‹ä»£ç ï¼š**

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# åˆ›å»ºç¤ºä¾‹æ•°æ®
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# æŸ¥çœ‹æ¨¡å‹å‚æ•°
print(f"æƒé‡ (coef_): {model.coef_[0][0]:.4f}")
print(f"åç½® (intercept_): {model.intercept_[0]:.4f}")

# è¿›è¡Œé¢„æµ‹
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"\nè®­ç»ƒé›† MSE: {train_mse:.4f}")
print(f"æµ‹è¯•é›† MSE: {test_mse:.4f}")
print(f"è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
print(f"æµ‹è¯•é›† RÂ²: {test_r2:.4f}")

# å¯è§†åŒ–ç»“æœ
plt.figure(figsize=(12, 4))

# è®­ç»ƒæ•°æ®
plt.subplot(1, 2, 1)
plt.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®')
plt.plot(X_train, y_train_pred, 'r-', linewidth=2, label='æ‹Ÿåˆç›´çº¿')
plt.xlabel('X')
plt.ylabel('y')
plt.title('è®­ç»ƒé›†æ‹Ÿåˆç»“æœ')
plt.legend()
plt.grid(True, linestyle=':', alpha=0.5)

# æµ‹è¯•æ•°æ®
plt.subplot(1, 2, 2)
plt.scatter(X_test, y_test, alpha=0.6, label='æµ‹è¯•æ•°æ®', color='green')
plt.plot(X_test, y_test_pred, 'r-', linewidth=2, label='æ‹Ÿåˆç›´çº¿')
plt.xlabel('X')
plt.ylabel('y')
plt.title('æµ‹è¯•é›†é¢„æµ‹ç»“æœ')
plt.legend()
plt.grid(True, linestyle=':', alpha=0.5)

plt.tight_layout()
plt.show()

# å¤šå…ƒçº¿æ€§å›å½’ç¤ºä¾‹
print("\n" + "="*40)
print("å¤šå…ƒçº¿æ€§å›å½’ç¤ºä¾‹")
print("="*40)

from sklearn.datasets import make_regression

# åˆ›å»ºå¤šå…ƒå›å½’æ•°æ®
X_multi, y_multi = make_regression(
    n_samples=100, n_features=3, noise=10, random_state=42
)

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X_multi, y_multi, test_size=0.2, random_state=42
)

# è®­ç»ƒæ¨¡å‹
model_multi = LinearRegression()
model_multi.fit(X_train, y_train)

# è¯„ä¼°
y_pred = model_multi.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"å„ç‰¹å¾çš„æƒé‡: {model_multi.coef_}")
print(f"åç½®: {model_multi.intercept_:.4f}")
print(f"æµ‹è¯•é›† MSE: {mse:.4f}")
print(f"æµ‹è¯•é›† RÂ²: {r2:.4f}")
```

**è§£é‡Šï¼š**

- `LinearRegression()`ï¼šåˆ›å»ºçº¿æ€§å›å½’æ¨¡å‹
- `fit(X, y)`ï¼šè®­ç»ƒæ¨¡å‹
- `predict(X)`ï¼šè¿›è¡Œé¢„æµ‹
- `coef_`ï¼šå­˜å‚¨æƒé‡å‚æ•°
- `intercept_`ï¼šå­˜å‚¨åç½®å‚æ•°
- `train_test_split()`ï¼šåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†

### çŸ¥è¯†ç‚¹ 5ï¼šæ¨¡å‹è¯„ä¼°

**æè¿°ï¼š**

è¯„ä¼°å›å½’æ¨¡å‹çš„æ€§èƒ½éœ€è¦ä½¿ç”¨åˆé€‚çš„æŒ‡æ ‡ã€‚å¸¸ç”¨çš„æŒ‡æ ‡åŒ…æ‹¬å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å’Œå†³å®šç³»æ•°ï¼ˆRÂ²ï¼‰ã€‚

**è¯„ä¼°æŒ‡æ ‡è¯¦è§£ï¼š**

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# å‡è®¾æœ‰çœŸå®å€¼å’Œé¢„æµ‹å€¼
y_true = np.array([3.0, -0.5, 2.0, 7.0])
y_pred = np.array([2.5, 0.0, 2.1, 7.8])

# 1. å‡æ–¹è¯¯å·® MSE (Mean Squared Error)
# å¯¹å¤§è¯¯å·®æ•æ„Ÿ
mse = mean_squared_error(y_true, y_pred)
print(f"MSE: {mse:.4f}")

# 2. å‡æ–¹æ ¹è¯¯å·® RMSE (Root Mean Squared Error)
# ä¸åŸå§‹æ•°æ®åŒå•ä½
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.4f}")

# 3. å¹³å‡ç»å¯¹è¯¯å·® MAE (Mean Absolute Error)
# å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
mae = mean_absolute_error(y_true, y_pred)
print(f"MAE: {mae:.4f}")

# 4. å†³å®šç³»æ•° RÂ² (R-squared)
# èŒƒå›´ [0, 1]ï¼Œè¶Šæ¥è¿‘ 1 æ‹Ÿåˆè¶Šå¥½
# 1 è¡¨ç¤ºå®Œç¾é¢„æµ‹ï¼Œ0 è¡¨ç¤ºå’Œç”¨å‡å€¼é¢„æµ‹ä¸€æ ·
r2 = r2_score(y_true, y_pred)
print(f"RÂ²: {r2:.4f}")

# RÂ² çš„è¯¦ç»†è§£é‡Š
# RÂ² = 1 - (SS_res / SS_tot)
# SS_res = sum((y_true - y_pred)^2)  æ®‹å·®å¹³æ–¹å’Œ
# SS_tot = sum((y_true - y_mean)^2)   æ€»å¹³æ–¹å’Œ

y_mean = np.mean(y_true)
ss_res = np.sum((y_true - y_pred) ** 2)
ss_tot = np.sum((y_true - y_mean) ** 2)
r2_manual = 1 - (ss_res / ss_tot)
print(f"RÂ² (æ‰‹åŠ¨è®¡ç®—): {r2_manual:.4f}")
```

**æŒ‡æ ‡é€‰æ‹©å»ºè®®ï¼š**

| æŒ‡æ ‡ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| MSE | æ•°å­¦æ€§è´¨å¥½ï¼Œå¯å¯¼ | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ | å¤§æ ·æœ¬ï¼Œå…³æ³¨å¤§è¯¯å·® |
| RMSE | ä¸ç›®æ ‡åŒå•ä½ï¼Œç›´è§‚ | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ | éœ€è¦ç›´è§‚è§£é‡Šè¯¯å·®å¤§å°æ—¶ |
| MAE | å¯¹å¼‚å¸¸å€¼é²æ£’ | ä¸å¯å¯¼ | æœ‰å¼‚å¸¸å€¼çš„åœºæ™¯ |
| RÂ² | ç›¸å¯¹æŒ‡æ ‡ï¼Œå¯æ¯”è¾ƒ | å¯èƒ½è¯¯å¯¼ | éœ€è¦è¯„ä¼°æ¨¡å‹æ•´ä½“è§£é‡ŠåŠ› |

### æœ¬ç« å°ç»“

æœ¬ç« å­¦ä¹ äº†çº¿æ€§å›å½’çš„å®Œæ•´æµç¨‹ï¼š

- **ç†è®ºåŸºç¡€**ï¼šå‡è®¾å‡½æ•°ã€æŸå¤±å‡½æ•°ã€æ¢¯åº¦ä¸‹é™
- **æ‰‹åŠ¨å®ç°**ï¼šä½¿ç”¨ NumPy ä»é›¶å®ç°çº¿æ€§å›å½’
- **ç‰¹å¾å·¥ç¨‹**ï¼šç‰¹å¾ç¼©æ”¾å¯¹æ¨¡å‹è®­ç»ƒçš„å½±å“
- **sklearn å®è·µ**ï¼šä½¿ç”¨ scikit-learn å¿«é€Ÿæ„å»ºæ¨¡å‹
- **æ¨¡å‹è¯„ä¼°**ï¼šMSEã€RMSEã€MAEã€RÂ² ç­‰è¯„ä¼°æŒ‡æ ‡

çº¿æ€§å›å½’æ˜¯æœºå™¨å­¦ä¹ çš„èµ·ç‚¹ï¼ŒæŒæ¡å®ƒçš„åŸç†å’Œå®ç°å°†ä¸ºå­¦ä¹ æ›´å¤æ‚çš„ç®—æ³•æ‰“ä¸‹åšå®åŸºç¡€ã€‚

### ğŸ““ ç¬”è®°æœ¬ç»ƒä¹ 

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œè¯·é€šè¿‡ä»¥ä¸‹ç¬”è®°æœ¬ç»ƒä¹ æ¥å·©å›ºä½ çš„çŸ¥è¯†ï¼š

- **[ç»ƒä¹ ç¬”è®°æœ¬](https://tiantianhang.github.io/python-teaching-platform/notebooks/index.html?fromURL=https://raw.githubusercontent.com/TianTianHang/python-teaching-platform/refs/heads/course-content-v1/courses/machine-learning-basics/notebooks/chapter-03-linear-regression-lab.ipynb)** - çº¿æ€§å›å½’å®è·µç»ƒä¹ 

> ğŸ’¡ **æç¤º**ï¼šå®Œæˆç»ƒä¹ åå¯ä»¥å‚è€ƒç­”æ¡ˆç¬”è®°æœ¬æ¥æ£€æŸ¥ä½ çš„ç­”æ¡ˆã€‚

- **[ç­”æ¡ˆç¬”è®°æœ¬](https://tiantianhang.github.io/python-teaching-platform/notebooks/index.html?fromURL=https://raw.githubusercontent.com/TianTianHang/python-teaching-platform/refs/heads/course-content-v1/courses/machine-learning-basics/solutions/chapter-03-linear-regression-lab-solution.ipynb)** - ç»ƒä¹ å‚è€ƒç­”æ¡ˆ

---
*æœ¬ç« å†…å®¹åŸºäº Python æ•™å­¦å¹³å°æ ‡å‡†æ ¼å¼è®¾è®¡ã€‚*
