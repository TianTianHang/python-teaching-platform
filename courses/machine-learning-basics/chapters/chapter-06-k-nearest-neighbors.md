---
title: "K è¿‘é‚»ç®—æ³•"
order: 6
unlock_conditions:
  type: "prerequisite"
  prerequisites: [5]
---
## K è¿‘é‚»ç®—æ³•

### ç« èŠ‚æ¦‚è¿°

K è¿‘é‚»ï¼ˆK-Nearest Neighborsï¼ŒKNNï¼‰æ˜¯ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯"è¿‘æœ±è€…èµ¤ï¼Œè¿‘å¢¨è€…é»‘"â€”â€”ä¸€ä¸ªæ ·æœ¬çš„ç±»åˆ«ç”±å®ƒæœ€è¿‘çš„ K ä¸ªé‚»å±…çš„ç±»åˆ«å†³å®šã€‚KNN æ˜¯ä¸€ç§æƒ°æ€§å­¦ä¹ ç®—æ³•ï¼Œä¸éœ€è¦æ˜¾å¼çš„è®­ç»ƒè¿‡ç¨‹ã€‚

### çŸ¥è¯†ç‚¹ 1ï¼šKNN ç®—æ³•åŸç†

**æè¿°ï¼š**

KNN æ˜¯ä¸€ç§åŸºäºå®ä¾‹çš„å­¦ä¹ ç®—æ³•ã€‚å¯¹äºæ–°çš„æµ‹è¯•æ ·æœ¬ï¼Œç®—æ³•ä¼šåœ¨è®­ç»ƒé›†ä¸­æ‰¾åˆ°è·ç¦»å®ƒæœ€è¿‘çš„ K ä¸ªæ ·æœ¬ï¼Œæ ¹æ®è¿™ K ä¸ªé‚»å±…çš„ç±»åˆ«æ¥é¢„æµ‹æ–°æ ·æœ¬çš„ç±»åˆ«ã€‚

**ç®—æ³•æ­¥éª¤ï¼š**

1. è®¡ç®—æµ‹è¯•æ ·æœ¬ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬ä¹‹é—´çš„è·ç¦»
2. æŒ‰è·ç¦»æ’åºï¼Œé€‰æ‹©æœ€è¿‘çš„ K ä¸ªæ ·æœ¬
3. ç»Ÿè®¡è¿™ K ä¸ªé‚»å±…ä¸­å„ç±»åˆ«çš„æ•°é‡
4. å°†æµ‹è¯•æ ·æœ¬å½’ç±»ä¸ºæ•°é‡æœ€å¤šçš„ç±»åˆ«

**åˆ†ç±» vs å›å½’ï¼š**

- **åˆ†ç±»é—®é¢˜**ï¼šK ä¸ªé‚»å±…ä¸­æœ€å¤šçš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç»“æœï¼ˆå¤šæ•°æŠ•ç¥¨ï¼‰
- **å›å½’é—®é¢˜**ï¼šK ä¸ªé‚»å±…çš„ç›®æ ‡å€¼å¹³å‡å€¼ä½œä¸ºé¢„æµ‹ç»“æœ

**ç¤ºä¾‹ä»£ç ï¼ˆæ‰‹åŠ¨å®ç°ï¼‰ï¼š**

```python
import numpy as np
from collections import Counter

def euclidean_distance(x1, x2):
    """
    è®¡ç®—æ¬§æ°è·ç¦»

    Args:
        x1, x2: ä¸¤ä¸ªæ ·æœ¬ç‚¹

    Returns:
        æ¬§æ°è·ç¦»
    """
    return np.sqrt(np.sum((x1 - x2) ** 2))

def knn_predict(X_train, y_train, x_test, k=3):
    """
    KNN åˆ†ç±»é¢„æµ‹

    Args:
        X_train: è®­ç»ƒé›†ç‰¹å¾ï¼Œå½¢çŠ¶ (m, n)
        y_train: è®­ç»ƒé›†æ ‡ç­¾ï¼Œå½¢çŠ¶ (m,)
        x_test: æµ‹è¯•æ ·æœ¬ï¼Œå½¢çŠ¶ (n,)
        k: æœ€è¿‘çš„é‚»å±…æ•°é‡

    Returns:
        é¢„æµ‹ç±»åˆ«
    """
    # 1. è®¡ç®—æµ‹è¯•æ ·æœ¬åˆ°æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»
    distances = []
    for i in range(len(X_train)):
        dist = euclidean_distance(x_test, X_train[i])
        distances.append((dist, y_train[i]))

    # 2. æŒ‰è·ç¦»æ’åº
    distances.sort(key=lambda x: x[0])

    # 3. é€‰æ‹©æœ€è¿‘çš„ K ä¸ªé‚»å±…
    k_nearest = distances[:k]
    k_labels = [label for _, label in k_nearest]

    # 4. å¤šæ•°æŠ•ç¥¨
    most_common = Counter(k_labels).most_common(1)
    return most_common[0][0]

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»ºç®€å•è®­ç»ƒæ•°æ®
    X_train = np.array([
        [1.0, 1.0],
        [1.5, 2.0],
        [2.0, 1.5],
        [6.0, 5.0],
        [7.0, 7.0],
        [8.0, 6.0]
    ])
    y_train = np.array([0, 0, 0, 1, 1, 1])

    # æµ‹è¯•æ ·æœ¬
    x_test = np.array([3.0, 3.0])

    # ä½¿ç”¨ä¸åŒçš„ K å€¼é¢„æµ‹
    for k in [1, 3, 5]:
        prediction = knn_predict(X_train, y_train, x_test, k)
        print(f"K={k} æ—¶ï¼Œé¢„æµ‹ç±»åˆ«: {prediction}")
```

**è§£é‡Šï¼š**

- KNN ä¸éœ€è¦è®­ç»ƒè¿‡ç¨‹ï¼Œç›´æ¥åœ¨é¢„æµ‹æ—¶è®¡ç®—è·ç¦»
- K çš„é€‰æ‹©å¯¹ç»“æœå½±å“å¾ˆå¤§
- è·ç¦»åº¦é‡é€šå¸¸ä½¿ç”¨æ¬§æ°è·ç¦»ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æ›¼å“ˆé¡¿è·ç¦»ç­‰

### çŸ¥è¯†ç‚¹ 2ï¼šè·ç¦»åº¦é‡

**æè¿°ï¼š**

KNN çš„æ ¸å¿ƒæ˜¯è®¡ç®—æ ·æœ¬é—´çš„è·ç¦»ã€‚ä¸åŒçš„è·ç¦»åº¦é‡é€‚åˆä¸åŒç±»å‹çš„æ•°æ®ã€‚

**å¸¸ç”¨è·ç¦»åº¦é‡ï¼š**

**1. æ¬§æ°è·ç¦»ï¼ˆEuclidean Distanceï¼‰**

$$d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$$

æœ€å¸¸è§çš„è·ç¦»åº¦é‡ï¼Œå³ä¸¤ç‚¹é—´çš„ç›´çº¿è·ç¦»ã€‚

**2. æ›¼å“ˆé¡¿è·ç¦»ï¼ˆManhattan Distanceï¼‰**

$$d(x, y) = \sum_{i=1}^{n}|x_i - y_i|$$

åŸå¸‚è¡—åŒºè·ç¦»ï¼Œé€‚åˆç½‘æ ¼çŠ¶è·¯å¾„ã€‚

**ç¤ºä¾‹ä»£ç ï¼š**

```python
import numpy as np

def euclidean_distance(x1, x2):
    """æ¬§æ°è·ç¦»"""
    return np.sqrt(np.sum((x1 - x2) ** 2))

def manhattan_distance(x1, x2):
    """æ›¼å“ˆé¡¿è·ç¦»"""
    return np.sum(np.abs(x1 - x2))

def minkowski_distance(x1, x2, p=3):
    """
    é—µå¯å¤«æ–¯åŸºè·ç¦»ï¼ˆMinkowski Distanceï¼‰
    p=1: æ›¼å“ˆé¡¿è·ç¦»
    p=2: æ¬§æ°è·ç¦»
    """
    return np.sum(np.abs(x1 - x2) ** p) ** (1 / p)

# æµ‹è¯•ä¸åŒè·ç¦»
point_a = np.array([1, 2, 3])
point_b = np.array([4, 5, 6])

print(f"æ¬§æ°è·ç¦»: {euclidean_distance(point_a, point_b):.4f}")
print(f"æ›¼å“ˆé¡¿è·ç¦»: {manhattan_distance(point_a, point_b):.4f}")
print(f"é—µå¯å¤«æ–¯åŸºè·ç¦» (p=3): {minkowski_distance(point_a, point_b, p=3):.4f}")
```

**è·ç¦»åº¦é‡é€‰æ‹©ï¼š**

| åº¦é‡ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| æ¬§æ°è·ç¦» | è€ƒè™‘ç»å¯¹å·®å¼‚ï¼Œå¯¹å¼‚å¸¸å€¼æ•æ„Ÿ | è¿ç»­æ•°å€¼ç‰¹å¾ |
| æ›¼å“ˆé¡¿è·ç¦» | å¯¹å¼‚å¸¸å€¼è¾ƒé²æ£’ | é«˜ç»´ç¨€ç–æ•°æ® |
| é—µå¯å¤«æ–¯åŸºè·ç¦» | é€šç”¨å½¢å¼ï¼Œp å¯è°ƒ | éœ€è¦çµæ´»è°ƒæ•´ |

### çŸ¥è¯†ç‚¹ 3ï¼šscikit-learn KNN åˆ†ç±»

**æè¿°ï¼š**

scikit-learn æä¾›äº†é«˜æ•ˆçš„ KNN å®ç°ï¼Œä½¿ç”¨ `KNeighborsClassifier` è¿›è¡Œåˆ†ç±»ã€‚

**ç¤ºä¾‹ä»£ç ï¼š**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import make_classification

# åˆ›å»ºç¤ºä¾‹æ•°æ®
X, y = make_classification(
    n_samples=300,
    n_features=2,
    n_redundant=0,
    n_informative=2,
    random_state=42,
    n_clusters_per_class=1,
    class_sep=1.5
)

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# æµ‹è¯•ä¸åŒ K å€¼çš„æ•ˆæœ
k_values = [1, 3, 5, 11, 21, 51]

plt.figure(figsize=(12, 8))

for idx, k in enumerate(k_values):
    # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    # åˆ›å»ºå­å›¾
    plt.subplot(2, 3, idx + 1)

    # ç»˜åˆ¶è®­ç»ƒæ•°æ®
    plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],
                c='blue', marker='o', label='ç±»åˆ« 0', alpha=0.5)
    plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],
                c='red', marker='s', label='ç±»åˆ« 1', alpha=0.5)

    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, alpha=0.3, levels=[-1, 0, 1, 2], colors=['blue', 'red'])
    plt.contour(xx, yy, Z, colors='black', linewidths=0.5)

    plt.xlabel('ç‰¹å¾ 1')
    plt.ylabel('ç‰¹å¾ 2')
    plt.title(f'K={k}\nå‡†ç¡®ç‡: {accuracy:.4f}')
    plt.grid(True, linestyle=':', alpha=0.3)

    if idx == 0:
        plt.legend()

plt.tight_layout()
plt.show()

# å¯»æ‰¾æœ€ä½³ K å€¼
print("\nå¯»æ‰¾æœ€ä½³ K å€¼:")
k_range = range(1, 51)
train_scores = []
test_scores = []

for k in k_range:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    train_scores.append(model.score(X_train, y_train))
    test_scores.append(model.score(X_test, y_test))

best_k = k_range[np.argmax(test_scores)]
best_score = max(test_scores)
print(f"æœ€ä½³ K å€¼: {best_k}")
print(f"æœ€ä½³æµ‹è¯•é›†å‡†ç¡®ç‡: {best_score:.4f}")

# ç»˜åˆ¶ K å€¼ä¸å‡†ç¡®ç‡çš„å…³ç³»
plt.figure(figsize=(10, 5))
plt.plot(k_range, train_scores, 'o-', label='è®­ç»ƒé›†å‡†ç¡®ç‡')
plt.plot(k_range, test_scores, 's-', label='æµ‹è¯•é›†å‡†ç¡®ç‡')
plt.axvline(x=best_k, color='green', linestyle='--', label=f'æœ€ä½³ K={best_k}')
plt.xlabel('K å€¼')
plt.ylabel('å‡†ç¡®ç‡')
plt.title('K å€¼å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“')
plt.legend()
plt.grid(True, linestyle=':', alpha=0.5)
plt.show()
```

**è§£é‡Šï¼š**

- `n_neighbors`ï¼šæŒ‡å®šé‚»å±…æ•°é‡ K
- K è¾ƒå°ï¼šæ¨¡å‹å¤æ‚ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆï¼Œå†³ç­–è¾¹ç•Œä¸è§„åˆ™
- K è¾ƒå¤§ï¼šæ¨¡å‹ç®€å•ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆï¼Œå†³ç­–è¾¹ç•Œå¹³æ»‘
- é€šå¸¸é€‰æ‹© K ä¸ºå¥‡æ•°ï¼Œé¿å…æŠ•ç¥¨å¹³å±€
- ç»éªŒæ³•åˆ™ï¼šK â‰ˆ âˆšnï¼ˆn ä¸ºæ ·æœ¬æ•°ï¼‰

### çŸ¥è¯†ç‚¹ 4ï¼šKNN å›å½’

**æè¿°ï¼š**

KNN ä¹Ÿå¯ä»¥ç”¨äºå›å½’é—®é¢˜ï¼Œé¢„æµ‹å€¼ä¸º K ä¸ªæœ€è¿‘é‚»ç›®æ ‡å€¼çš„å¹³å‡å€¼ï¼ˆæˆ–åŠ æƒå¹³å‡ï¼‰ã€‚

**ç¤ºä¾‹ä»£ç ï¼š**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# åˆ›å»ºå›å½’æ•°æ®
np.random.seed(42)
X = np.random.rand(200, 1) * 10
y = np.sin(X).ravel() + np.random.randn(200) * 0.2

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# æµ‹è¯•ä¸åŒ K å€¼
k_values = [1, 5, 10, 20]

plt.figure(figsize=(12, 3))

for idx, k in enumerate(k_values):
    # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
    model = KNeighborsRegressor(n_neighbors=k)
    model.fit(X_train, y_train)

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # ç»˜åˆ¶ç»“æœ
    plt.subplot(1, 4, idx + 1)

    # ç»˜åˆ¶è®­ç»ƒæ•°æ®
    plt.scatter(X_train, y_train, c='blue', alpha=0.5, s=20, label='è®­ç»ƒæ•°æ®')

    # ç»˜åˆ¶æµ‹è¯•æ•°æ®
    plt.scatter(X_test, y_test, c='green', alpha=0.5, s=20, label='æµ‹è¯•æ•°æ®')

    # ç»˜åˆ¶é¢„æµ‹æ›²çº¿
    X_plot = np.linspace(0, 10, 200).reshape(-1, 1)
    y_plot = model.predict(X_plot)
    plt.plot(X_plot, y_plot, 'r-', linewidth=2, label='KNN é¢„æµ‹')

    plt.xlabel('X')
    plt.ylabel('y')
    plt.title(f'K={k}\nMSE: {mse:.4f}, RÂ²: {r2:.4f}')
    plt.grid(True, linestyle=':', alpha=0.5)

    if idx == 0:
        plt.legend(fontsize=8)

plt.tight_layout()
plt.show()
```

**KNN å›å½’ç‰¹ç‚¹ï¼š**

- é¢„æµ‹å€¼æ˜¯é‚»å±…ç›®æ ‡å€¼çš„å¹³å‡å€¼
- åŠ æƒ KNN å›å½’ï¼šè·ç¦»è¶Šè¿‘çš„é‚»å±…æƒé‡è¶Šå¤§
- `weights='distance'` å‚æ•°å¯å¯ç”¨åŠ æƒæ¨¡å¼

### çŸ¥è¯†ç‚¹ 5ï¼šKNN ä¼˜ç¼ºç‚¹

**æè¿°ï¼š**

äº†è§£ KNN çš„ä¼˜ç¼ºç‚¹æœ‰åŠ©äºåœ¨å®é™…é¡¹ç›®ä¸­åšå‡ºåˆé€‚çš„é€‰æ‹©ã€‚

**ä¼˜ç‚¹ï¼š**

1. **ç®€å•ç›´è§‚**ï¼šæ˜“äºç†è§£å’Œå®ç°
2. **æ— éœ€è®­ç»ƒ**ï¼šæ˜¯ä¸€ç§æƒ°æ€§å­¦ä¹ ç®—æ³•
3. **é€‚ç”¨æ€§å¹¿**ï¼šå¯ç”¨äºåˆ†ç±»å’Œå›å½’
4. **å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ**ï¼ˆK > 1 æ—¶ï¼‰
5. **å¤šåˆ†ç±»å‹å¥½**ï¼šå¤©ç„¶æ”¯æŒå¤šåˆ†ç±»é—®é¢˜

**ç¼ºç‚¹ï¼š**

1. **è®¡ç®—æˆæœ¬é«˜**ï¼šé¢„æµ‹æ—¶éœ€è¦è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»
2. **å­˜å‚¨éœ€æ±‚å¤§**ï¼šéœ€è¦å­˜å‚¨æ‰€æœ‰è®­ç»ƒæ•°æ®
3. **å¯¹ç»´åº¦æ•æ„Ÿ**ï¼šé«˜ç»´æ•°æ®å®¹æ˜“å‡ºç°"ç»´åº¦ç¾éš¾"
4. **éœ€è¦é€‰æ‹©åˆé€‚çš„ K å€¼**ï¼šK å€¼å¯¹ç»“æœå½±å“å¤§
5. **å¯¹æ•°æ®ä¸å¹³è¡¡æ•æ„Ÿ**ï¼šå¤šæ•°ç±»å¯èƒ½ä¸»å¯¼é¢„æµ‹

**ä¼˜åŒ–å»ºè®®ï¼š**

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1. ç‰¹å¾ç¼©æ”¾ï¼šKNN å¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. é™ç»´ï¼šç¼“è§£ç»´åº¦ç¾éš¾
pca = PCA(n_components=0.95)  # ä¿ç•™ 95% æ–¹å·®
X_reduced = pca.fit_transform(X_scaled)

# 3. ä½¿ç”¨ KD æ ‘æˆ– Ball æ ‘åŠ é€Ÿï¼ˆscikit-learn è‡ªåŠ¨é€‰æ‹©ï¼‰
model = KNeighborsClassifier(
    n_neighbors=5,
    algorithm='auto',  # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•
    # algorithm='kd_tree'  # ä¹Ÿå¯ä»¥æŒ‡å®š
)

# 4. åŠ æƒ KNNï¼šè·ç¦»è¿‘çš„é‚»å±…æƒé‡æ›´å¤§
model_weighted = KNeighborsClassifier(
    n_neighbors=5,
    weights='distance'  # é»˜è®¤æ˜¯ 'uniform'
)
```

**KD æ ‘ vs Ball æ ‘ï¼š**

| ç®—æ³• | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| Brute Force | æš´åŠ›æœç´¢ï¼Œè®¡ç®—æ‰€æœ‰è·ç¦» | å°æ•°æ®é›† |
| KD Tree | äºŒå‰æ ‘ç»“æ„ï¼Œå¿«é€Ÿæœç´¢ | ä½ç»´æ•°æ®ï¼ˆ< 20 ç»´ï¼‰ |
| Ball Tree | è¶…çƒä½“åˆ’åˆ†ï¼Œæ›´é€‚åˆé«˜ç»´ | ä¸­é«˜ç»´æ•°æ® |

### æœ¬ç« å°ç»“

æœ¬ç« å­¦ä¹ äº† K è¿‘é‚»ç®—æ³•ï¼š

- **ç®—æ³•åŸç†**ï¼šåŸºäºæœ€è¿‘é‚»å±…çš„å¤šæ•°æŠ•ç¥¨
- **è·ç¦»åº¦é‡**ï¼šæ¬§æ°è·ç¦»ã€æ›¼å“ˆé¡¿è·ç¦»ã€é—µå¯å¤«æ–¯åŸºè·ç¦»
- **K å€¼é€‰æ‹©**ï¼šå½±å“æ¨¡å‹å¤æ‚åº¦å’Œæ³›åŒ–èƒ½åŠ›
- **åˆ†ç±»ä¸å›å½’**ï¼šKNN å¯ç”¨äºä¸¤ç±»é—®é¢˜
- **ä¼˜ç¼ºç‚¹**ï¼šç®€å•ä½†è®¡ç®—æˆæœ¬é«˜

KNN æ˜¯æœºå™¨å­¦ä¹ å…¥é—¨çš„é‡è¦ç®—æ³•ï¼Œè™½ç„¶ç®€å•ï¼Œä½†åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­æ•ˆæœè‰¯å¥½ã€‚ç†è§£ KNN çš„å·¥ä½œåŸç†æœ‰åŠ©äºç†è§£æ›´å¤æ‚çš„ç®—æ³•ã€‚

### ğŸ““ ç¬”è®°æœ¬ç»ƒä¹ 

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œè¯·é€šè¿‡ä»¥ä¸‹ç¬”è®°æœ¬ç»ƒä¹ æ¥å·©å›ºä½ çš„çŸ¥è¯†ï¼š

- **[ç»ƒä¹ ç¬”è®°æœ¬](https://tiantianhang.github.io/python-teaching-platform/notebooks/index.html?fromURL=https://raw.githubusercontent.com/TianTianHang/python-teaching-platform/refs/heads/course-content-v1/courses/machine-learning-basics/notebooks/chapter-06-knn-lab.ipynb)** - K è¿‘é‚»ç®—æ³•å®è·µç»ƒä¹ 

> ğŸ’¡ **æç¤º**ï¼šå®Œæˆç»ƒä¹ åå¯ä»¥å‚è€ƒç­”æ¡ˆç¬”è®°æœ¬æ¥æ£€æŸ¥ä½ çš„ç­”æ¡ˆã€‚

- **[ç­”æ¡ˆç¬”è®°æœ¬](https://tiantianhang.github.io/python-teaching-platform/notebooks/index.html?fromURL=https://raw.githubusercontent.com/TianTianHang/python-teaching-platform/refs/heads/course-content-v1/courses/machine-learning-basics/solutions/chapter-06-knn-lab-solution.ipynb)** - ç»ƒä¹ å‚è€ƒç­”æ¡ˆ

---
*æœ¬ç« å†…å®¹åŸºäº Python æ•™å­¦å¹³å°æ ‡å‡†æ ¼å¼è®¾è®¡ã€‚*
