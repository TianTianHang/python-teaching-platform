{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: K 近邻算法 (KNN) 实践\n",
    "\n",
    "## 实验概述\n",
    "\n",
    "在本实验中，你将使用 **Digits 数据集**（手写数字识别）来学习 K 近邻算法的实现和应用。Digits 数据集包含 1797 个 8x8 像素的手写数字图像，每个图像对应 0-9 中的一个数字。\n",
    "\n",
    "### 实验目标\n",
    "\n",
    "1. 理解 KNN 算法的基本原理\n",
    "2. 实现欧氏距离计算\n",
    "3. 手动实现 KNN 分类器\n",
    "4. 使用 scikit-learn 的 KNN 模型\n",
    "5. 找到最优的 K 值\n",
    "6. 可视化决策边界和混淆矩阵\n",
    "\n",
    "### 数据集信息\n",
    "\n",
    "- **样本数量**: 1797\n",
    "- **特征数量**: 64 (8x8 像素)\n",
    "- **类别数量**: 10 (数字 0-9)\n",
    "- **特征**: 每个特征的值是 0-16 之间的灰度值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 数据加载与探索\n",
    "\n",
    "### 1.1 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"库导入成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 加载 Digits 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 加载数据集\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(\"数据集信息:\")\n",
    "print(f\"样本数量: {X.shape[0]}\")\n",
    "print(f\"特征数量: {X.shape[1]}\")\n",
    "print(f\"图像尺寸: 8x8\")\n",
    "print(f\"类别数量: {len(np.unique(y))}\")\n",
    "print(f\"\\n类别: {np.unique(y)}\")\n",
    "print(f\"\\n特征值范围: [{X.min():.1f}, {X.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 可视化手写数字图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 可视化前 20 个数字图像\n",
    "fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(20):\n",
    "    axes[i].imshow(digits.images[i], cmap='gray')\n",
    "    axes[i].set_title(f'标签: {y[i]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Digits 数据集 - 前20个样本', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 数据集统计\n",
    "\n",
    "**任务**: 统计每个数字的样本数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\n# 统计每个数字的样本数量\n# 使用 np.bincount() 统计每个类别的样本数\nlabel_counts = np.bincount(y)\n\nprint(\"每个数字的样本数量:\")\nfor digit, count in enumerate(label_counts):\n    print(f\"  数字 {digit}: {count} 个样本\")\n\n# 可视化类别分布\nplt.figure(figsize=(10, 5))\nbars = plt.bar(range(10), label_counts, color='skyblue', edgecolor='black', alpha=0.7)\n\n# 在柱子上方添加数值标签\nfor bar, count in zip(bars, label_counts):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n             f'{count}', ha='center', va='bottom', fontweight='bold')\n\nplt.xlabel('数字')\nplt.ylabel('样本数量')\nplt.title('Digits 数据集 - 类别分布')\nplt.xticks(range(10))\nplt.grid(True, alpha=0.3, axis='y')\n\n# 添加平均线\nmean_count = label_counts.mean()\nplt.axhline(y=mean_count, color='red', linestyle='--', \n            label=f'平均: {mean_count:.0f}')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# 统计信息\nprint(f\"\\n统计信息:\")\nprint(f\"总样本数: {len(y)}\")\nprint(f\"最少类别样本数: {label_counts.min()}\")\nprint(f\"最多类别样本数: {label_counts.max()}\")\nprint(f\"标准差: {label_counts.std():.2f}\")\nprint(f\"数据集是否平衡: {'是' if label_counts.std() < 20 else '否'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 数据预处理与划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 特征归一化 (0-1 范围)\n",
    "X_normalized = X / 16.0  # 像素值范围是 0-16\n",
    "\n",
    "# 划分数据集 (80% 训练, 20% 测试)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {X_train.shape}\")\n",
    "print(f\"测试集大小: {X_test.shape}\")\n",
    "print(f\"\\n特征值范围 (归一化后): [{X_normalized.min():.1f}, {X_normalized.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 距离度量实现\n",
    "\n",
    "### 2.1 欧氏距离\n",
    "\n",
    "**任务**: 实现欧氏距离函数 $d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\ndef euclidean_distance(x1, x2):\n    \"\"\"\n    计算两个向量之间的欧氏距离\n    \n    参数:\n    x1: 第一个向量 (n_features,)\n    x2: 第二个向量 (n_features,)\n    \n    返回:\n    欧氏距离\n    \"\"\"\n    # 计算欧氏距离：sqrt(sum((x1 - x2)^2))\n    return np.sqrt(np.sum((x1 - x2) ** 2))\n\n# 测试欧氏距离函数\na = np.array([1, 2, 3, 4])\nb = np.array([5, 6, 7, 8])\ndist = euclidean_distance(a, b)\nprint(f\"欧氏距离测试: d({a}, {b}) = {dist:.4f}\")\nprint(f\"手动计算: sqrt((1-5)^2 + (2-6)^2 + (3-7)^2 + (4-8)^2) = sqrt(16+16+16+16) = sqrt(64) = 8.0\")\n# 预期输出: 8.0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 曼哈顿距离\n",
    "\n",
    "**任务**: 实现曼哈顿距离函数 $d(x, y) = \\sum_{i=1}^{n}|x_i - y_i|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\ndef manhattan_distance(x1, x2):\n    \"\"\"\n    计算两个向量之间的曼哈顿距离\n    \n    参数:\n    x1: 第一个向量\n    x2: 第二个向量\n    \n    返回:\n    曼哈顿距离\n    \"\"\"\n    # 计算曼哈顿距离：sum(|x1 - x2|)\n    return np.sum(np.abs(x1 - x2))\n\n# 测试曼哈顿距离函数\ndist_m = manhattan_distance(a, b)\nprint(f\"曼哈顿距离测试: d({a}, {b}) = {dist_m:.4f}\")\nprint(f\"手动计算: |1-5| + |2-6| + |3-7| + |4-8| = 4 + 4 + 4 + 4 = 16.0\")\n# 预期输出: 16.0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 批量距离计算\n",
    "\n",
    "**任务**: 计算一个样本到所有训练样本的距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\ndef compute_distances(X_train, x_test, metric='euclidean'):\n    \"\"\"\n    计算测试样本到所有训练样本的距离\n    \n    参数:\n    X_train: 训练集 (n_samples, n_features)\n    x_test: 测试样本 (n_features,)\n    metric: 距离度量 ('euclidean' 或 'manhattan')\n    \n    返回:\n    距离数组 (n_samples,)\n    \"\"\"\n    distances = []\n    \n    # 根据选择的距离度量函数\n    if metric == 'euclidean':\n        distance_func = euclidean_distance\n    elif metric == 'manhattan':\n        distance_func = manhattan_distance\n    else:\n        raise ValueError(\"不支持的距离度量\")\n    \n    # 遍历训练集，计算每个样本到测试样本的距离\n    for x_train in X_train:\n        dist = distance_func(x_train, x_test)\n        distances.append(dist)\n    \n    return np.array(distances)\n\n# 测试\nx_sample = X_test[0]\ndists = compute_distances(X_train, x_sample, metric='euclidean')\nprint(f\"测试样本到训练集的距离 (前10个): {dists[:10]}\")\nprint(f\"最小距离: {dists.min():.4f}\")\nprint(f\"最大距离: {dists.max():.4f}\")\nprint(f\"平均距离: {dists.mean():.4f}\")\n\n# 测试曼哈顿距离\ndists_m = compute_distances(X_train, x_sample, metric='manhattan')\nprint(f\"\\n曼哈顿距离 - 前10个: {dists_m[:10]}\")\nprint(f\"曼哈顿距离 - 最小: {dists_m.min():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 手动实现 KNN 分类器\n",
    "\n",
    "### 3.1 实现预测函数\n",
    "\n",
    "**任务**: 实现 KNN 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\ndef knn_predict(X_train, y_train, x_test, k=5):\n    \"\"\"\n    KNN 分类预测\n    \n    参数:\n    X_train: 训练集特征\n    y_train: 训练集标签\n    x_test: 测试样本\n    k: 邻居数量\n    \n    返回:\n    预测类别\n    \"\"\"\n    # 1. 计算测试样本到所有训练样本的距离\n    distances = compute_distances(X_train, x_test, metric='euclidean')\n    \n    # 2. 获取距离最近的 k 个样本的索引\n    k_indices = np.argsort(distances)[:k]\n    \n    # 3. 获取 k 个最近邻的标签\n    k_nearest_labels = y_train[k_indices]\n    \n    # 4. 多数投票，返回最常见的类别\n    label_counts = Counter(k_nearest_labels)\n    prediction = label_counts.most_common(1)[0][0]\n    \n    return prediction\n\n# 测试 KNN 预测\nx_test_sample = X_test[0]\ntrue_label = y_test[0]\npred_label = knn_predict(X_train, y_train, x_test_sample, k=5)\n\nprint(f\"测试样本索引: 0\")\nprint(f\"真实标签: {true_label}\")\nprint(f\"预测标签: {pred_label}\")\nprint(f\"预测{'正确' if true_label == pred_label else '错误'}!\")\n\n# 显示5个最近邻的标签\ndistances = compute_distances(X_train, x_test_sample, metric='euclidean')\nk_indices = np.argsort(distances)[:5]\nk_nearest_labels = y_train[k_indices]\n\nprint(f\"\\n5个最近邻的标签: {k_nearest_labels}\")\nprint(f\"邻居距离: {distances[k_indices]}\")\nprint(f\"投票结果: {Counter(k_nearest_labels)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 批量预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\ndef knn_predict_batch(X_train, y_train, X_test, k=5):\n    \"\"\"\n    对测试集进行批量预测\n    \n    参数:\n    X_train: 训练集特征\n    y_train: 训练集标签\n    X_test: 测试集特征\n    k: 邻居数量\n    \n    返回:\n    预测标签数组\n    \"\"\"\n    predictions = []\n    \n    # 遍历测试集，对每个样本进行预测\n    for i, x_test in enumerate(X_test):\n        pred = knn_predict(X_train, y_train, x_test, k)\n        predictions.append(pred)\n        \n        # 每100个样本打印一次进度\n        if (i + 1) % 100 == 0:\n            print(f\"已处理 {i+1}/{len(X_test)} 个样本\")\n    \n    return np.array(predictions)\n\n# 在小测试集上测试（只取前20个样本加快速度）\nprint(\"在测试集子集上进行预测（前20个样本）...\")\ny_pred_manual = knn_predict_batch(X_train, y_train, X_test[:20], k=5)\n\naccuracy = np.mean(y_pred_manual == y_test[:20])\nprint(f\"\\n手动实现的 KNN 准确率: {accuracy:.4f}\")\n\n# 显示详细的预测结果\nprint(\"\\n详细预测结果:\")\nfor i in range(len(y_test[:20])):\n    print(f\"样本 {i}: 真实={y_test[i]}, 预测={y_pred_manual[i]}, {'✓' if y_test[i] == y_pred_manual[i] else '✗'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 使用 scikit-learn 的 KNN\n",
    "\n",
    "### 4.1 创建 KNN 分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# 创建 KNN 分类器\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# 训练模型（注意：KNN 是惰性学习，训练过程只是存储数据）\nknn.fit(X_train, y_train)\n\n# 在训练集和测试集上预测\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\n\n# 计算准确率\ntrain_acc = accuracy_score(y_train, y_train_pred)\ntest_acc = accuracy_score(y_test, y_test_pred)\n\nprint(f\"KNN 分类器 (K=5):\")\nprint(f\"  训练集准确率: {train_acc:.4f}\")\nprint(f\"  测试集准确率: {test_acc:.4f}\")\nprint(f\"  过拟合情况: {'是' if train_acc - test_acc > 0.05 else '否'}\")\n\n# 显示部分预测结果\nprint(\"\\n前10个测试样本的预测结果:\")\nfor i in range(10):\n    print(f\"样本 {i}: 真实={y_test[i]}, 预测={y_test_pred[i]}, {'✓' if y_test[i] == y_test_pred[i] else '✗'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 寻找最优 K 值\n",
    "\n",
    "**任务**: 尝试不同的 K 值，找到最优的 K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\n# 尝试不同的 K 值\nk_values = list(range(1, 31, 2))  # 1, 3, 5, ..., 29\ntrain_scores = []\ntest_scores = []\n\n# 遍历不同的 K 值，记录训练集和测试集的准确率\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    \n    train_acc = knn.score(X_train, y_train)\n    test_acc = knn.score(X_test, y_test)\n    \n    train_scores.append(train_acc)\n    test_scores.append(test_acc)\n    \n    print(f\"K={k:2d}: 训练准确率={train_acc:.4f}, 测试准确率={test_acc:.4f}\")\n\n# 找到最佳 K 值（测试集准确率最高）\nbest_k_idx = np.argmax(test_scores)\nbest_k = k_values[best_k_idx]\nbest_score = test_scores[best_k_idx]\n\nprint(f\"\\n最佳 K 值: {best_k}\")\nprint(f\"最佳测试集准确率: {best_score:.4f}\")\n\n# 绘制 K 值与准确率的关系\nplt.figure(figsize=(12, 6))\nplt.plot(k_values, train_scores, 'o-', label='训练集准确率', linewidth=2, markersize=6)\nplt.plot(k_values, test_scores, 's-', label='测试集准确率', linewidth=2, markersize=6)\nplt.axvline(x=best_k, color='green', linestyle='--', linewidth=2, label=f'最佳 K={best_k}')\nplt.xlabel('K 值', fontsize=12)\nplt.ylabel('准确率', fontsize=12)\nplt.title('K 值对模型性能的影响', fontsize=14)\nplt.legend(fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.xticks(k_values)\n\n# 在图中标注关键信息\nplt.annotate(f'最高测试准确率\\n{best_score:.3f}', \n             xy=(best_k, best_score),\n             xytext=(best_k + 5, best_score - 0.02),\n             arrowprops=dict(arrowstyle='->', color='red'),\n             fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n# 分析结果\nprint(\"\\n分析结果:\")\nprint(f\"K=1 时的训练准确率: {train_scores[0]:.4f}\")\nprint(f\"K=29 时的训练准确率: {train_scores[-1]:.4f}\")\nprint(f\"K=1 时的测试准确率: {test_scores[0]:.4f}\")\nprint(f\"K=29 时的测试准确率: {test_scores[-1]:.4f}\")\nprint(f\"\\n训练准确率下降: {train_scores[0] - train_scores[-1]:.4f}\")\nprint(f\"测试准确率变化: {test_scores[0] - test_scores[-1]:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 加权 KNN\n",
    "\n",
    "**任务**: 比较均匀权重和距离权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\n# 创建两个 KNN 模型：均匀权重和距离权重\n\n# 均匀权重：所有邻居的权重相同\nknn_uniform = KNeighborsClassifier(n_neighbors=best_k, weights='uniform')\n\n# 距离权重：距离越近的邻居权重越大\n# 权重 = 1 / distance\nknn_distance = KNeighborsClassifier(n_neighbors=best_k, weights='distance')\n\n# 训练并评估\nknn_uniform.fit(X_train, y_train)\nknn_distance.fit(X_train, y_train)\n\nuniform_acc = knn_uniform.score(X_test, y_test)\ndistance_acc = knn_distance.score(X_test, y_test)\n\nprint(\"权重策略比较:\")\nprint(f\"  均匀权重 (uniform): {uniform_acc:.4f}\")\nprint(f\"  距离权重 (distance): {distance_acc:.4f}\")\nprint(f\"  差异: {distance_acc - uniform_acc:+.4f}\")\n\n# 分析\nif distance_acc > uniform_acc:\n    print(\"\\n结论: 距离权重表现更好，说明近邻样本更重要\")\nelif distance_acc < uniform_acc:\n    print(\"\\n结论: 均匀权重表现更好，可能是噪声样本影响\")\nelse:\n    print(\"\\n结论: 两种策略效果相当\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 不同距离度量的比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 比较不同的距离度量\n",
    "metrics = ['euclidean', 'manhattan', 'minkowski']\n",
    "results = []\n",
    "\n",
    "for metric in metrics:\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_k, metric=metric)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = knn.score(X_train, y_train)\n",
    "    test_acc = knn.score(X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'metric': metric,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"不同距离度量的比较:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: 模型评估与可视化\n",
    "\n",
    "### 5.1 使用最佳模型进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\n# 使用最佳参数创建模型\nbest_knn = KNeighborsClassifier(n_neighbors=best_k)\n\n# 训练模型\nbest_knn.fit(X_train, y_train)\n\n# 进行预测\ny_pred_best = best_knn.predict(X_test)\n\n# 显示分类报告\nprint(\"分类报告:\")\nprint(classification_report(y_test, y_pred_best, digits=4))\n\nprint(f\"测试集准确率: {accuracy_score(y_test, y_pred_best):.4f}\")\n\n# 显示每个类别的准确率\nfor digit in range(10):\n    mask = y_test == digit\n    if mask.sum() > 0:\n        acc_digit = accuracy_score(y_test[mask], y_pred_best[mask])\n        print(f\"  数字 {digit} 的准确率: {acc_digit:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 混淆矩阵可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\n# 计算混淆矩阵\ncm = confusion_matrix(y_test, y_pred_best)\n\n# 可视化混淆矩阵\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=range(10), yticklabels=range(10))\nplt.xlabel('预测类别')\nplt.ylabel('真实类别')\nplt.title('混淆矩阵 - 手写数字识别')\nplt.show()\n\n# 分析混淆矩阵\nprint(\"混淆矩阵分析:\")\nprint(f\"对角线元素和 (正确预测数): {np.trace(cm)}\")\nprint(f\"总样本数: {cm.sum()}\")\nprint(f\"准确率: {np.trace(cm) / cm.sum():.4f}\")\n\n# 找出最容易混淆的数字对\nprint(\"\\n最易混淆的数字对 (非对角线元素):\")\nconfusion_pairs = []\nfor i in range(10):\n    for j in range(10):\n        if i != j and cm[i, j] > 0:\n            confusion_pairs.append((i, j, cm[i, j]))\n\n# 按混淆数量排序\nconfusion_pairs.sort(key=lambda x: x[2], reverse=True)\nfor real, pred, count in confusion_pairs[:5]:\n    print(f\"  数字 {real} 被预测为 {pred}: {count} 次\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 可视化预测结果\n",
    "\n",
    "**任务**: 显示一些预测正确和错误的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\n# 找出预测正确和错误的样本\ncorrect_mask = y_pred_best == y_test\ncorrect_indices = np.where(correct_mask)[0]\nincorrect_indices = np.where(~correct_mask)[0]\n\nprint(f\"预测正确数量: {len(correct_indices)}\")\nprint(f\"预测错误数量: {len(incorrect_indices)}\")\nprint(f\"准确率: {len(correct_indices) / (len(correct_indices) + len(incorrect_indices)):.4f}\")\n\n# 可视化一些预测结果\nfig, axes = plt.subplots(2, 5, figsize=(14, 6))\n\n# 第一行: 预测正确的样本\nfor i in range(5):\n    idx = correct_indices[i]\n    img = X_test[idx].reshape(8, 8)\n    axes[0, i].imshow(img, cmap='gray')\n    axes[0, i].set_title(f'真实:{y_test[idx]}, 预测:{y_pred_best[idx]}')\n    axes[0, i].axis('off')\n\n# 第二行: 预测错误的样本\nfor i in range(5):\n    if i < len(incorrect_indices):\n        idx = incorrect_indices[i]\n        img = X_test[idx].reshape(8, 8)\n        axes[1, i].imshow(img, cmap='gray')\n        axes[1, i].set_title(f'真:{y_test[idx]}, 预:{y_pred_best[idx]}', color='red')\n        axes[1, i].axis('off')\n\naxes[0, 0].set_ylabel('预测正确', fontsize=12)\naxes[1, 0].set_ylabel('预测错误', fontsize=12)\nplt.suptitle('KNN 预测结果示例', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# 分析错误预测\nprint(\"\\n错误预测分析:\")\nfor i in range(min(5, len(incorrect_indices))):\n    idx = incorrect_indices[i]\n    print(f\"样本 {idx}: 真实={y_test[idx]}, 预测={y_pred_best[idx]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: 挑战练习\n",
    "\n",
    "### 6.1 交叉验证\n",
    "\n",
    "**任务**: 使用交叉验证评估模型稳定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\n# 使用 5 折交叉验证评估模型\ncv_scores = cross_val_score(\n    KNeighborsClassifier(n_neighbors=best_k),\n    X_normalized, y,\n    cv=5,\n    scoring='accuracy'\n)\n\nprint(\"5 折交叉验证结果:\")\nprint(f\"每折分数: {cv_scores}\")\nprint(f\"平均分数: {cv_scores.mean():.4f}\")\nprint(f\"标准差: {cv_scores.std():.4f}\")\nprint(f\"最小值: {cv_scores.min():.4f}\")\nprint(f\"最大值: {cv_scores.max():.4f}\")\nprint(f\"变异系数: {cv_scores.std() / cv_scores.mean() * 100:.2f}%\")\n\n# 可视化\nplt.figure(figsize=(10, 5))\nbars = plt.bar(range(1, 6), cv_scores, alpha=0.7, edgecolor='black', \n               color='steelblue', linewidth=2)\nplt.axhline(y=cv_scores.mean(), color='red', linestyle='--', linewidth=2,\n            label=f'平均分: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}')\nplt.xlabel('折数', fontsize=12)\nplt.ylabel('准确率', fontsize=12)\nplt.title('5 折交叉验证结果', fontsize=14)\nplt.xticks(range(1, 6))\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3, axis='y')\nplt.ylim([cv_scores.min() - 0.02, cv_scores.max() + 0.02])\n\n# 在柱子上添加数值\nfor bar, score in zip(bars, cv_scores):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n             f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# 稳定性分析\nprint(\"\\n稳定性分析:\")\nif cv_scores.std() < 0.01:\n    print(\"模型非常稳定，各折之间差异很小\")\nelif cv_scores.std() < 0.02:\n    print(\"模型稳定，各折之间差异较小\")\nelse:\n    print(\"模型稳定性一般，各折之间有较大差异\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 决策边界可视化（简化版）\n",
    "\n",
    "**任务**: 使用 PCA 降维后可视化决策边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== 参考答案 =====\nfrom sklearn.decomposition import PCA\n\n# 使用 PCA 将数据降维到 2D\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train)\nX_test_2d = pca.transform(X_test)\n\nprint(f\"PCA 解释方差比: {pca.explained_variance_ratio_}\")\nprint(f\"累积解释方差: {pca.explained_variance_ratio_.sum():.4f}\")\n\n# 在 2D 数据上训练 KNN\nknn_2d = KNeighborsClassifier(n_neighbors=best_k)\nknn_2d.fit(X_train_2d, y_train)\n\n# 计算边界\nx_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1\ny_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                     np.linspace(y_min, y_max, 200))\n\n# 预测网格\nZ = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# 绘制决策边界\nplt.figure(figsize=(14, 6))\n\n# 左图: 训练集\nplt.subplot(1, 2, 1)\nscatter = plt.scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train,\n                       cmap='tab10', alpha=0.6, s=30, edgecolors='black', linewidth=0.5)\nplt.contourf(xx, yy, Z, alpha=0.3, cmap='tab10')\nplt.xlabel('主成分 1')\nplt.ylabel('主成分 2')\nplt.title(f'训练集 - PCA 降维后的决策边界 (K={best_k})')\nplt.colorbar(scatter, label='数字')\n\n# 右图: 测试集\nplt.subplot(1, 2, 2)\nscatter = plt.scatter(X_test_2d[:, 0], X_test_2d[:, 1], c=y_test,\n                       cmap='tab10', alpha=0.6, s=30, edgecolors='black', linewidth=0.5)\nplt.contourf(xx, yy, Z, alpha=0.3, cmap='tab10')\nplt.xlabel('主成分 1')\nplt.ylabel('主成分 2')\nplt.title(f'测试集 - PCA 降维后的决策边界 (K={best_k})')\nplt.colorbar(scatter, label='数字')\n\nplt.tight_layout()\nplt.show()\n\n# 评估 2D 模型的性能\ntrain_acc_2d = knn_2d.score(X_train_2d, y_train)\ntest_acc_2d = knn_2d.score(X_test_2d, y_test)\nprint(f\"\\n2D PCA 模型性能:\")\nprint(f\"训练集准确率: {train_acc_2d:.4f}\")\nprint(f\"测试集准确率: {test_acc_2d:.4f}\")\nprint(f\"\\n注意: PCA 降维后损失了部分信息，准确率会降低\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 性能分析\n",
    "\n",
    "**任务**: 分析 K 值对预测时间的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 测试不同 K 值的预测时间\n",
    "k_values = [1, 5, 11, 21, 31, 51]\n",
    "prediction_times = []\n",
    "\n",
    "print(\"测量不同 K 值的预测时间...\\n\")\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # 测量预测时间\n",
    "    start = time.time()\n",
    "    knn.predict(X_test)\n",
    "    end = time.time()\n",
    "    \n",
    "    elapsed = end - start\n",
    "    prediction_times.append(elapsed)\n",
    "    print(f\"K={k:2d}: 预测时间 = {elapsed:.4f} 秒\")\n",
    "\n",
    "# 绘制结果\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_values, prediction_times, 'o-', linewidth=2)\n",
    "plt.xlabel('K 值')\n",
    "plt.ylabel('预测时间 (秒)')\n",
    "plt.title('K 值对预测时间的影响')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n结论:\")\n",
    "print(\"- KNN 的预测时间随着 K 值增加略有增加\")\n",
    "print(\"- KNN 的主要时间开销是距离计算，这与 K 值关系不大\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "恭喜你完成了 KNN 实验！在本实验中，你学习了：\n",
    "\n",
    "1. ✅ **KNN 原理**: 基于最近邻居的多数投票\n",
    "2. ✅ **距离度量**: 欧氏距离、曼哈顿距离\n",
    "3. ✅ **手动实现**: 从零实现 KNN 分类器\n",
    "4. ✅ **scikit-learn KNN**: 使用 KNeighborsClassifier\n",
    "5. ✅ **K 值选择**: 找到最优的邻居数量\n",
    "6. ✅ **模型评估**: 准确率、混淆矩阵、交叉验证\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "- **K 值选择**: K 太小容易过拟合，K 太大容易欠拟合\n",
    "- **距离度量**: 欧氏距离最常用，曼哈顿距离对异常值更鲁棒\n",
    "- **加权投票**: 距离权重通常比均匀权重效果好\n",
    "- **特征缩放**: KNN 对特征尺度敏感，需要归一化\n",
    "- **计算成本**: KNN 是惰性学习，预测时需要计算大量距离\n",
    "\n",
    "### 进一步学习\n",
    "\n",
    "- 尝试其他距离度量（如余弦相似度）\n",
    "- 学习 KD 树和 Ball 树等加速算法\n",
    "- 探索 KNN 在回归问题中的应用 (KNeighborsRegressor)\n",
    "- 研究维度灾难及其缓解方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}