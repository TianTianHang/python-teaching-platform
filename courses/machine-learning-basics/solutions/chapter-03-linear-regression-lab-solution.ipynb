{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: 线性回归实践\n",
    "\n",
    "## 实验概述\n",
    "\n",
    "在本实验中，你将使用 **Diabetes 数据集**来学习线性回归的实现和应用。Diabetes 数据集包含 442 个糖尿病患者的数据，每个患者有 10 个生理特征，目标是预测一年后疾病进展的定量测量值。\n",
    "\n",
    "### 实验目标\n",
    "\n",
    "1. 理解线性回归的基本原理\n",
    "2. 实现梯度下降算法\n",
    "3. 手动实现线性回归模型\n",
    "4. 使用 scikit-learn 的线性回归模型\n",
    "5. 进行特征标准化和正则化\n",
    "6. 评估模型性能\n",
    "\n",
    "### 数据集信息\n",
    "\n",
    "- **样本数量**: 442\n",
    "- **特征数量**: 10\n",
    "- **特征**: 年龄、性别、BMI、血压、血清测量值等\n",
    "- **目标**: 一年后疾病进展的定量测量值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 数据加载与探索\n",
    "\n",
    "### 1.1 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"库导入成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 加载 Diabetes 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 加载数据集\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "feature_names = diabetes.feature_names\n",
    "\n",
    "print(\"数据集信息:\")\n",
    "print(f\"样本数量: {X.shape[0]}\")\n",
    "print(f\"特征数量: {X.shape[1]}\")\n",
    "print(f\"\\n特征名称:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {i+1}. {name}\")\n",
    "print(f\"\\n目标值范围: [{y.min():.1f}, {y.max():.1f}]\")\n",
    "print(f\"目标值均值: {y.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 数据探索\n",
    "\n",
    "**任务**: 创建 DataFrame 并查看数据统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 创建 DataFrame\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# TODO: 添加目标列 'progression'\n",
    "df['progression'] = y\n",
    "\n",
    "print(\"前5行数据:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n数据描述性统计:\")\n",
    "# TODO: 显示描述性统计信息\n",
    "# 提示: 使用 describe() 方法\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 数据可视化\n",
    "\n",
    "**任务**: 绘制 BMI 与疾病进展的关系图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# TODO: 绘制散点图\n",
    "# 提示: plt.scatter(x, y, alpha=0.5)\n",
    "plt.scatter(df['bmi'], df['progression'], alpha=0.5)\n",
    "\n",
    "# 添加回归线\n",
    "# 暂时不添加，等后面实现后再添加\n",
    "\n",
    "plt.xlabel('BMI (体重指数)')\n",
    "plt.ylabel('疾病进展 (定量测量值)')\n",
    "plt.title('BMI 与疾病进展的关系')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 划分数据集 (80% 训练, 20% 测试)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {X_train.shape}\")\n",
    "print(f\"测试集大小: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 特征预处理\n",
    "\n",
    "### 2.1 特征标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 创建标准化器\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 标准化训练集和测试集\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"标准化前的统计信息 (年龄特征):\")\n",
    "print(f\"训练集均值: {X_train[:, 0].mean():.2f}, 标准差: {X_train[:, 0].std():.2f}\")\n",
    "print(f\"测试集均值: {X_test[:, 0].mean():.2f}, 标准差: {X_test[:, 0].std():.2f}\")\n",
    "\n",
    "print(\"\\n标准化后的统计信息 (年龄特征):\")\n",
    "print(f\"训练集均值: {X_train_scaled[:, 0].mean():.2f}, 标准差: {X_train_scaled[:, 0].std():.2f}\")\n",
    "print(f\"测试集均值: {X_test_scaled[:, 0].mean():.2f}, 标准差: {X_test_scaled[:, 0].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 手动实现线性回归\n",
    "\n",
    "### 3.1 实现线性回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, theta):\n",
    "    \"\"\"\n",
    "    线性回归前向传播\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)，包含截距项\n",
    "    theta: 参数向量 (n+1,)\n",
    "    \n",
    "    返回:\n",
    "    预测值 (m,)\n",
    "    \"\"\"\n",
    "    # TODO: 计算预测值\n",
    "    # 提示: 使用矩阵乘法 np.dot(X, theta)\n",
    "    return np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 实现损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(X, y, theta):\n",
    "    \"\"\"\n",
    "    计算均方误差损失\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵\n",
    "    y: 真实值\n",
    "    theta: 参数\n",
    "    \n",
    "    返回:\n",
    "    均方误差\n",
    "    \"\"\"\n",
    "    # TODO 1: 计算预测值\n",
    "    y_pred = linear_regression(X, theta)\n",
    "    \n",
    "    # TODO 2: 计算均方误差\n",
    "    # MSE = (1/m) * sum((y_pred - y)^2)\n",
    "    m = len(y)\n",
    "    mse = (1/m) * np.sum((y_pred - y) ** 2)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# 测试损失函数\n",
    "X_train_with_intercept = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "initial_theta = np.zeros(X_train_scaled.shape[1] + 1)\n",
    "test_mse = compute_mse(X_train_with_intercept, y_train, initial_theta)\n",
    "print(f\"初始损失函数值: {test_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 实现梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    计算梯度\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)\n",
    "    y: 真实值 (m,)\n",
    "    theta: 参数 (n+1,)\n",
    "    \n",
    "    返回:\n",
    "    梯度 (n+1,)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    \n",
    "    # TODO 1: 计算预测值\n",
    "    y_pred = linear_regression(X, theta)\n",
    "    \n",
    "    # TODO 2: 计算误差\n",
    "    error = y_pred - y\n",
    "    \n",
    "    # TODO 3: 计算梯度\n",
    "    # gradient = (1/m) * X^T @ (y_pred - y)\n",
    "    gradient = (1/m) * np.dot(X.T, error)\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# 测试梯度计算\n",
    "test_gradient = compute_gradient(X_train_with_intercept, y_train, initial_theta)\n",
    "print(f\"梯度形状: {test_gradient.shape}\")\n",
    "print(f\"梯度前5个值: {test_gradient[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 实现梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    梯度下降算法\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵\n",
    "    y: 真实值\n",
    "    theta: 初始参数\n",
    "    alpha: 学习率\n",
    "    num_iterations: 迭代次数\n",
    "    \n",
    "    返回:\n",
    "    优化后的参数和损失历史\n",
    "    \"\"\"\n",
    "    # 添加截距项\n",
    "    m = len(y)\n",
    "    X_with_intercept = np.c_[np.ones((m, 1)), X]\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # TODO 1: 计算当前损失\n",
    "        current_loss = compute_mse(X_with_intercept, y, theta)\n",
    "        loss_history.append(current_loss)\n",
    "        \n",
    "        # TODO 2: 计算梯度\n",
    "        gradient = compute_gradient(X_with_intercept, y, theta)\n",
    "        \n",
    "        # TODO 3: 更新参数\n",
    "        theta = theta - alpha * gradient\n",
    "        \n",
    "        # 每100次迭代打印一次损失\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"迭代 {i+1}/{num_iterations}, 损失: {current_loss:.4f}\")\n",
    "    \n",
    "    return theta, loss_history\n",
    "\n",
    "# 测试梯度下降\n",
    "alpha = 0.01  # 学习率\n",
    "num_iterations = 1000  # 迭代次数\n",
    "\n",
    "initial_theta = np.zeros(X_train_scaled.shape[1] + 1)\n",
    "\n",
    "print(\"开始梯度下降...\")\n",
    "theta, loss_history = gradient_descent(X_train_scaled, y_train, initial_theta, alpha, num_iterations)\n",
    "print(f\"\\n优化后的参数:\")\n",
    "print(f\"截距: {theta[0]:.4f}\")\n",
    "for i, coef in enumerate(theta[1:], 1):\n",
    "    print(f\"{feature_names[i-1]}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 可视化损失曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('损失 (MSE)')\n",
    "plt.title('梯度下降损失曲线')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 模型预测与评估\n",
    "\n",
    "### 4.1 实现预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    使用训练好的模型进行预测\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n)\n",
    "    theta: 参数向量 (n+1,)\n",
    "    \n",
    "    返回:\n",
    "    预测值 (m,)\n",
    "    \"\"\"\n",
    "    # TODO: 添加截距项并计算预测值\n",
    "    X_with_intercept = np.c_[np.ones((len(X), 1)), X]\n",
    "    y_pred = linear_regression(X_with_intercept, theta)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_test_pred_manual = predict(X_test_scaled, theta)\n",
    "\n",
    "# 计算均方误差\n",
    "mse_manual = mean_squared_error(y_test, y_test_pred_manual)\n",
    "\n",
    "    # 计算R²分数\n",
    "r2_manual = r2_score(y_test, y_test_pred_manual)\n",
    "\n",
    "print(f\"手动实现的线性回归:\")\n",
    "print(f\"  测试集 MSE: {mse_manual:.4f}\")\n",
    "print(f\"  测试集 R²: {r2_manual:.4f}\")\n",
    "\n",
    "    # 绘制预测 vs 真实值\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_test_pred_manual, alpha=0.5)\n",
    "\n",
    "    # 绘制完美预测线\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "\n",
    "plt.xlabel('真实值')\n",
    "plt.ylabel('预测值')\n",
    "plt.title('真实值 vs 预测值')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: 使用 scikit-learn 的线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 创建并训练 scikit-learn 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# TODO: 创建线性回归模型\n",
    "sklearn_lr = LinearRegression()\n",
    "\n",
    "# TODO: 训练模型\n",
    "sklearn_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# TODO: 在测试集上预测\n",
    "y_test_pred_sklearn = sklearn_lr.predict(X_test_scaled)\n",
    "\n",
    "# 计算指标\n",
    "mse_sklearn = mean_squared_error(y_test, y_test_pred_sklearn)\n",
    "r2_sklearn = r2_score(y_test, y_test_pred_sklearn)\n",
    "\n",
    "print(f\"scikit-learn 线性回归:\")\n",
    "print(f\"  测试集 MSE: {mse_sklearn:.4f}\")\n",
    "print(f\"  测试集 R²: {r2_sklearn:.4f}\")\n",
    "\n",
    "# 比较两种方法的结果\n",
    "print(f\"\\n两种方法比较:\")\n",
    "print(f\"  MSE 差值: {abs(mse_manual - mse_sklearn):.4f}\")\n",
    "print(f\"  R² 差值: {abs(r2_manual - r2_sklearn):.4f}\")\n",
    "\n",
    "# 显示 sklearn 的系数\n",
    "print(f\"\\nsklearn 回归系数:\")\n",
    "print(f\"截距: {sklearn_lr.intercept_:.4f}\")\n",
    "for i, coef in enumerate(sklearn_lr.coef_):\n",
    "    print(f\"{feature_names[i]}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: 挑战练习\n",
    "\n",
    "### 6.1 实现 L2 正则化（Ridge 回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_gradient_descent(X, y, theta, alpha, lambda_reg, num_iterations):\n",
    "    \"\"\"\n",
    "    带L2正则化的梯度下降\n",
    "    \n",
    "    参数:\n",
    "    lambda_reg: L2正则化系数\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    X_with_intercept = np.c_[np.ones((m, 1)), X]\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # 计算当前损失\n",
    "        y_pred = linear_regression(X_with_intercept, theta)\n",
    "        mse = (1/(2*m)) * np.sum((y_pred - y) ** 2)\n",
    "        # 添加L2正则项\n",
    "        l2_reg = (lambda_reg / (2*m)) * np.sum(theta[1:] ** 2)\n",
    "        total_loss = mse + l2_reg\n",
    "        loss_history.append(total_loss)\n",
    "        \n",
    "        # 计算梯度\n",
    "        gradient = (1/m) * X_with_intercept.T @ (y_pred - y)\n",
    "        # 添加L2正则的梯度（不惩罚截距项）\n",
    "        gradient[1:] += (lambda_reg/m) * theta[1:]\n",
    "        \n",
    "        # 更新参数\n",
    "        theta = theta - alpha * gradient\n",
    "    \n",
    "    return theta, loss_history\n",
    "\n",
    "# 尝试不同的正则化系数\n",
    "lambda_values = [0.01, 0.1, 1.0, 10.0]\n",
    "results = []\n",
    "\n",
    "print(\"Ridge 回归正则化效果:\")\n",
    "for lambda_reg in lambda_values:\n",
    "    theta_ridge, _ = ridge_regression_gradient_descent(\n",
    "        X_train_scaled, y_train, initial_theta, alpha, lambda_reg, num_iterations\n",
    "    )\n",
    "    \n",
    "    y_pred_ridge = predict(X_test_scaled, theta_ridge)\n",
    "    mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "    \n",
    "    results.append({\n",
    "        'lambda': lambda_reg,\n",
    "        'mse': mse_ridge\n",
    "    })\n",
    "    \n",
    "    print(f\"lambda={lambda_reg}: MSE = {mse_ridge:.4f}\")\n",
    "\n",
    "# 找到最佳正则化系数\n",
    "best_result = min(results, key=lambda x: x['mse'])\n",
    "print(f\"\\n最佳 lambda: {best_result['lambda']}, MSE: {best_result['mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 特征重要性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取 sklearn 模型的系数（不包括截距）\n",
    "feature_importance = np.abs(sklearn_lr.coef_)\n",
    "\n",
    "# 创建 DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': sklearn_lr.coef_,\n",
    "    'absolute_importance': feature_importance\n",
    "}).sort_values('absolute_importance', ascending=False)\n",
    "\n",
    "print(\"特征重要性（基于系数绝对值）:\")\n",
    "print(importance_df)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['coefficient'])\n",
    "plt.xlabel('系数值')\n",
    "plt.ylabel('特征')\n",
    "plt.title('线性回归特征系数')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 多项式特征扩展"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 选择最重要的两个特征进行可视化\n",
    "important_features = ['bmi', 'bp']\n",
    "feature_indices = [i for i, name in enumerate(feature_names) if name in important_features]\n",
    "\n",
    "# 创建多项式特征（2阶）\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled[:, feature_indices])\n",
    "X_test_poly = poly.transform(X_test_scaled[:, feature_indices])\n",
    "\n",
    "# 训练多项式回归模型\n",
    "poly_lr = LinearRegression()\n",
    "poly_lr.fit(X_train_poly, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_poly = poly_lr.predict(X_test_poly)\n",
    "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(f\"多项式回归 (degree=2):\")\n",
    "print(f\"  测试集 MSE: {mse_poly:.4f}\")\n",
    "print(f\"  测试集 R²: {r2_poly:.4f}\")\n",
    "\n",
    "print(f\"\\n与线性回归比较:\")\n",
    "print(f\"  MSE 改善: {mse_sklearn - mse_poly:.4f}\")\n",
    "print(f\"  R² 改善: {r2_poly - r2_sklearn:.4f}\")\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_poly, alpha=0.5, label='多项式回归')\n",
    "plt.scatter(y_test, y_test_pred_sklearn, alpha=0.5, label='线性回归')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "plt.xlabel('真实值')\n",
    "plt.ylabel('预测值')\n",
    "plt.title('线性回归 vs 多项式回归')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "恭喜你完成了线性回归实验！在本实验中，你学习了：\n",
    "\n",
    "1. ✅ **线性回归原理**: 最小二乘法、梯度下降\n",
    "2. ✅ **手动实现**: 从零实现梯度下降算法\n",
    "3. ✅ **scikit-learn**: 使用 LinearRegression 类\n",
    "4. ✅ **特征预处理**: 标准化的重要性\n",
    "5. ✅ **模型评估**: MSE、R² 指标\n",
    "6. ✅ **正则化**: L2 正则化（Ridge 回归）\n",
    "7. ✅ **多项式特征**: 非线性扩展\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "- **梯度下降**: 通过迭代更新参数来最小化损失函数\n",
    "- **学习率**: α 值的选择影响收敛速度和稳定性\n",
    "- **特征缩放**: 标准化使梯度下降更快收敛\n",
    "- **正则化**: 防止过拟合，提高模型泛化能力\n",
    "- **评估指标**: MSE 衡量误差，R² 衡量解释方差比例\n",
    "\n",
    "### 进一步学习\n",
    "\n",
    "- 尝试其他正则化方法（L1 正则化 - Lasso）\n",
    "- 学习岭回归（Ridge）和弹性网络（ElasticNet）\n",
    "- 探索随机梯度下降（SGD）的变体\n",
    "- 学习多项式回归的最佳阶数选择"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}