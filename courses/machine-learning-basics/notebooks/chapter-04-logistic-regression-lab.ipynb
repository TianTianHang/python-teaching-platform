{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: 逻辑回归实践\n",
    "\n",
    "## 实验概述\n",
    "\n",
    "在本实验中，你将使用 **Breast Cancer 数据集**来学习逻辑回归的实现和应用。这个数据集包含 569 个乳腺肿瘤样本，每个样本有 30 个特征，目标是判断肿瘤是良性还是恶性。\n",
    "\n",
    "### 实验目标\n",
    "\n",
    "1. 理解逻辑回归的原理\n",
    "2. 实现 Sigmoid 函数\n",
    "3. 实现二分类交叉熵损失\n",
    "4. 手动实现逻辑回归模型\n",
    "5. 使用 scikit-learn 的逻辑回归\n",
    "6. 绘制 ROC 曲线和混淆矩阵\n",
    "\n",
    "### 数据集信息\n",
    "\n",
    "- **样本数量**: 569\n",
    "- **特征数量**: 30\n",
    "- **特征**: 肿瘤尺寸、纹理、平滑度、对称性等\n",
    "- **目标**: 0=良性, 1=恶性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 数据加载与探索\n",
    "\n",
    "### 1.1 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \n",
    "classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"库导入成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 加载 Breast Cancer 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 加载数据集\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "feature_names = cancer.feature_names\n",
    "target_names = cancer.target_names\n",
    "\n",
    "print(\"数据集信息:\")\n",
    "print(f\"样本数量: {X.shape[0]}\")\n",
    "print(f\"特征数量: {X.shape[1]}\")\n",
    "print(f\"类别数量: {len(np.unique(y))}\")\n",
    "print(f\"类别名称: {target_names}\")\n",
    "print(f\"良性样本: {np.sum(y == 0)} 个\")\n",
    "print(f\"恶性样本: {np.sum(y == 1)} 个\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 数据探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"前5行数据:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n数据统计信息:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 数据可视化\n",
    "\n",
    "**任务**: 查看几个重要特征的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "# 选择几个重要特征进行可视化\n",
    "important_features = ['mean radius', 'mean texture', 'mean smoothness', 'mean compactness']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(important_features):\n",
    "    # TODO: 绘制两个类别的分布\n",
    "    # 提示: 使用 sns.kdeplot() 或 plt.hist()\n",
    "    # malignant = df[df['target'] == 1]\n",
    "    # benign = df[df['target'] == 0]\n",
    "    pass\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 划分数据集 (80% 训练, 20% 测试)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {X_train.shape}\")\n",
    "print(f\"测试集大小: {X_test.shape}\")\n",
    "print(f\"\\n训练集类别分布: {np.bincount(y_train)}\")\n",
    "print(f\"测试集类别分布: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 特征标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 创建标准化器\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 标准化特征\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"标准化完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 核心函数实现\n",
    "\n",
    "### 2.1 Sigmoid 函数\n",
    "\n",
    "**任务**: 实现 Sigmoid 函数 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid 函数\n",
    "    \n",
    "    参数:\n",
    "    z: 输入值（可以是标量或数组）\n",
    "    \n",
    "    返回:\n",
    "    Sigmoid 函数的值（0到1之间）\n",
    "    \"\"\"\n",
    "    # TODO: 实现 Sigmoid 函数\n",
    "    # 避免数值溢出\n",
    "    # z = np.clip(z, -250, 250)  # 限制范围防止溢出\n",
    "    # return 1 / (1 + np.exp(-z))\n",
    "    pass\n",
    "\n",
    "# 测试 Sigmoid 函数\n",
    "z_values = np.array([-5, 0, 5])\n",
    "# sigmoid_results = sigmoid(z_values)\n",
    "print(f\"Sigmoid 函数测试:\")\n",
    "for z in z_values:\n",
    "    print(f\"  σ({z:.1f}) = \")\n",
    "print(f\"\\n当 z → -∞, σ(z) → \")\n",
    "print(f\"当 z → +∞, σ(z) → \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 实现前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "def forward_propagation(X, theta):\n",
    "    \"\"\"\n",
    "    逻辑回归前向传播\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)，包含截距项\n",
    "    theta: 参数向量 (n+1,)\n",
    "    \n",
    "    返回:\n",
    "    概率值 (m,)\n",
    "    \"\"\"\n",
    "    # TODO: 计算线性组合并应用 Sigmoid 函数\n",
    "    # z = np.dot(X, theta)\n",
    "    # return sigmoid(z)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 实现二分类交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算二分类交叉熵损失\n",
    "    \n",
    "    参数:\n",
    "    y_true: 真实标签 (0或1) (m,)\n",
    "    y_pred: 预测概率 (0到1之间) (m,)\n",
    "    \n",
    "    返回:\n",
    "    平均损失\n",
    "    \"\"\"\n",
    "    # 避免 log(0) 的情况\n",
    "    # y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    # TODO: 计算二分类交叉熵\n",
    "    # 当 y_true=1 时: -log(y_pred)\n",
    "    # 当 y_true=0 时: -log(1 - y_pred)\n",
    "    m = len(y_true)\n",
    "    # loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    # return loss\n",
    "\n",
    "# 测试交叉熵损失\n",
    "y_true = np.array([1, 0, 1, 1])\n",
    "y_pred = np.array([0.9, 0.1, 0.8, 0.7])\n",
    "# loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"交叉熵损失测试: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 实现梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "def compute_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    计算逻辑回归的梯度\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)\n",
    "    y: 真实标签 (m,)\n",
    "    theta: 参数 (n+1,)\n",
    "    \n",
    "    返回:\n",
    "    梯度 (n+1,)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    \n",
    "    # TODO 1: 计算预测概率\n",
    "    # y_pred = forward_propagation(X, theta)\n",
    "    \n",
    "    # TODO 2: 计算误差\n",
    "    # error = y_pred - y\n",
    "    \n",
    "    # TODO 3: 计算梯度\n",
    "    # gradient = (1/m) * X^T @ (y_pred - y)\n",
    "    # gradient = (1/m) * np.dot(X.T, error)\n",
    "    \n",
    "    # return gradient\n",
    "\n",
    "# 测试梯度计算\n",
    "X_train_with_intercept = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "test_theta = np.zeros(X_train_with_intercept.shape[1])\n",
    "# test_gradient = compute_gradient(X_train_with_intercept, y_train, test_theta)\n",
    "print(f\"梯度形状: {test_gradient.shape}\")\n",
    "print(f\"梯度前5个值: {test_gradient[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 手动实现逻辑回归\n",
    "\n",
    "### 3.1 实现梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "def logistic_regression_gradient_descent(X, y, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    逻辑回归梯度下降算法\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵\n",
    "    y: 真实标签\n",
    "    theta: 初始参数\n",
    "    alpha: 学习率\n",
    "    num_iterations: 迭代次数\n",
    "    \n",
    "    返回:\n",
    "    优化后的参数和损失历史\n",
    "    \"\"\"\n",
    "    # 添加截距项\n",
    "    m = len(y)\n",
    "    X_with_intercept = np.c_[np.ones((m, 1)), X]\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # TODO 1: 计算当前损失\n",
    "        # y_pred = forward_propagation(X_with_intercept, theta)\n",
    "        # current_loss = binary_cross_entropy(y, y_pred)\n",
    "        loss_history.append(current_loss)\n",
    "        \n",
    "        # TODO 2: 计算梯度\n",
    "        # gradient = compute_gradient(X_with_intercept, y, theta)\n",
    "        \n",
    "        # TODO 3: 更新参数\n",
    "        # theta = theta - alpha * gradient\n",
    "        \n",
    "        # 每100次迭代打印一次损失\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"迭代 {i+1}/{num_iterations}, 损失: {current_loss:.4f}\")\n",
    "    \n",
    "    # return theta, loss_history\n",
    "\n",
    "# 测试梯度下降\n",
    "alpha = 0.01  # 学习率\n",
    "num_iterations = 1000  # 迭代次数\n",
    "\n",
    "initial_theta = np.zeros(X_train_scaled.shape[1] + 1)\n",
    "\n",
    "print(\"开始逻辑回归梯度下降...\")\n",
    "# theta, loss_history = logistic_regression_gradient_descent(\n",
    "#     X_train_scaled, y_train, initial_theta, alpha, num_iterations\n",
    "# )\n",
    "\n",
    "print(f\"\\n优化后的参数:\")\n",
    "# print(f\"截距: {theta[0]:.4f}\")\n",
    "for i, coef in enumerate(theta[1:], 1):\n",
    "    print(f\"{feature_names[i-1]}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 可视化损失曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 预填充代码 =====\n",
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('损失 (交叉熵)')\n",
    "plt.title('逻辑回归损失曲线')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 实现预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "def predict_proba(X, theta):\n",
    "    \"\"\"\n",
    "    预测概率\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n)\n",
    "    theta: 参数向量 (n+1,)\n",
    "    \n",
    "    返回:\n",
    "    概率值 (m,)\n",
    "    \"\"\"\n",
    "    # TODO: 添加截距项并计算概率\n",
    "    # X_with_intercept = np.c_[np.ones((len(X), 1)), X]\n",
    "    # return forward_propagation(X_with_intercept, theta)\n",
    "    pass\n",
    "\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    \"\"\"\n",
    "    预测类别\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵\n",
    "    theta: 参数\n",
    "    threshold: 分类阈值\n",
    "    \n",
    "    返回:\n",
    "    预测类别 (0或1)\n",
    "    \"\"\"\n",
    "    # probs = predict_proba(X, theta)\n",
    "    # return (probs >= threshold).astype(int)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "# y_test_pred_manual = predict(X_test_scaled, theta)\n",
    "# y_test_pred_proba = predict_proba(X_test_scaled, theta)\n",
    "\n",
    "# 计算准确率\n",
    "# accuracy_manual = accuracy_score(y_test, y_test_pred_manual)\n",
    "\n",
    "print(f\"手动实现的逻辑回归:\")\n",
    "# print(f\"  测试集准确率: {accuracy_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 使用 scikit-learn 的逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 创建并训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# TODO: 创建逻辑回归模型\n",
    "# sklearn_lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# TODO: 训练模型\n",
    "# sklearn_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# TODO: 在测试集上预测\n",
    "# y_test_pred_sklearn = sklearn_lr.predict(X_test_scaled)\n",
    "# y_test_pred_proba_sklearn = sklearn_lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# 计算指标\n",
    "# accuracy_sklearn = accuracy_score(y_test, y_test_pred_sklearn)\n",
    "\n",
    "print(f\"scikit-learn 逻辑回归:\")\n",
    "# print(f\"  测试集准确率: {accuracy_sklearn:.4f}\")\n",
    "\n",
    "# 比较两种方法\n",
    "print(f\"\\n两种方法比较:\")\n",
    "# print(f\"  准确率差值: {abs(accuracy_manual - accuracy_sklearn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "# TODO: 计算混淆矩阵\n",
    "# cm_manual = None  # 替换为你的代码\n",
    "# cm_sklearn = None  # 替换为你的代码\n",
    "\n",
    "# 显示混淆矩阵\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 手动实现的混淆矩阵\n",
    "# sns.heatmap(cm_manual, annot=True, fmt='d', cmap='Blues', \n",
    "#             xticklabels=target_names, yticklabels=target_names, ax=axes[0])\n",
    "axes[0].set_title('手动实现的逻辑回归')\n",
    "axes[0].set_xlabel('预测类别')\n",
    "axes[0].set_ylabel('真实类别')\n",
    "\n",
    "# sklearn的混淆矩阵\n",
    "# sns.heatmap(cm_sklearn, annot=True, fmt='d', cmap='Blues',\n",
    "#             xticklabels=target_names, yticklabels=target_names, ax=axes[1])\n",
    "axes[1].set_title('scikit-learn 逻辑回归')\n",
    "axes[1].set_xlabel('预测类别')\n",
    "axes[1].set_ylabel('真实类别')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ROC 曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "# TODO: 计算手动实现的 ROC 曲线\n",
    "# fpr_manual = None  # 替换为你的代码\n",
    "# tpr_manual = None  # 替换为你的代码\n",
    "# roc_auc_manual = None  # 替换为你的代码\n",
    "\n",
    "# 计算 sklearn 的 ROC 曲线\n",
    "fpr_sklearn, tpr_sklearn, _ = roc_curve(y_test, y_test_pred_proba_sklearn)\n",
    "roc_auc_sklearn = auc(fpr_sklearn, tpr_sklearn)\n",
    "\n",
    "# 绘制 ROC 曲线\n",
    "plt.figure(figsize=(8, 6))\n",
    "# TODO: 绘制两条 ROC 曲线\n",
    "# plt.plot(fpr_manual, tpr_manual, label=f'手动实现 (AUC = {roc_auc_manual:.4f})', linewidth=2)\n",
    "plt.plot(fpr_sklearn, tpr_sklearn, label=f'sklearn (AUC = {roc_auc_sklearn:.4f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('假阳性率')\n",
    "plt.ylabel('真阳性率')\n",
    "plt.title('ROC 曲线比较')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: 挑战练习\n",
    "\n",
    "### 5.1 L2 正则化实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "def logistic_regression_with_l2(X, y, theta, alpha, lambda_reg, num_iterations):\n",
    "    \"\"\"\n",
    "    带L2正则化的逻辑回归\n",
    "    \n",
    "    参数:\n",
    "    lambda_reg: L2正则化系数\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    X_with_intercept = np.c_[np.ones((m, 1)), X]\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # 计算当前损失\n",
    "        # y_pred = forward_propagation(X_with_intercept, theta)\n",
    "        # cross_entropy = binary_cross_entropy(y, y_pred)\n",
    "        # 添加L2正则项（不惩罚截距项）\n",
    "        # l2_reg = (lambda_reg / (2*m)) * np.sum(theta[1:] ** 2)\n",
    "        # total_loss = cross_entropy + l2_reg\n",
    "        # loss_history.append(total_loss)\n",
    "        \n",
    "        # 计算梯度\n",
    "        # gradient = compute_gradient(X_with_intercept, y, theta)\n",
    "        # 添加L2正则的梯度\n",
    "        # gradient[1:] += (lambda_reg/m) * theta[1:]\n",
    "        \n",
    "        # 更新参数\n",
    "        # theta = theta - alpha * gradient\n",
    "    \n",
    "    # return theta, loss_history\n",
    "\n",
    "# 尝试不同的正则化系数\n",
    "lambda_values = [0.001, 0.01, 0.1, 1.0]\n",
    "results = []\n",
    "\n",
    "print(\"L2正则化效果:\")\n",
    "for lambda_reg in lambda_values:\n",
    "    # theta_l2, _ = logistic_regression_with_l2(\n",
    "    #     X_train_scaled, y_train, initial_theta, alpha, lambda_reg, num_iterations\n",
    "    # )\n",
    "    \n",
    "    # y_pred_l2 = predict(X_test_scaled, theta_l2)\n",
    "    # accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
    "    \n",
    "    # results.append({\n",
    "    #     'lambda': lambda_reg,\n",
    "    #     'accuracy': accuracy_l2\n",
    "    # })\n",
    "    \n",
    "    print(f\"lambda={lambda_reg}: 准确率 = {accuracy_l2:.4f}\")\n",
    "\n",
    "# 找到最佳正则化系数\n",
    "# best_result = max(results, key=lambda x: x['accuracy'])\n",
    "# print(f\"\\n最佳 lambda: {best_result['lambda']}, 准确率: {best_result['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 特征重要性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "# 提取 sklearn 模型的系数\n",
    "# feature_importance = np.abs(sklearn_lr.coef_[0])\n",
    "\n",
    "# 创建 DataFrame\n",
    "# importance_df = pd.DataFrame({\n",
    "#     'feature': feature_names,\n",
    "#     'coefficient': sklearn_lr.coef_[0],\n",
    "#     'absolute_importance': feature_importance\n",
    "# }).sort_values('absolute_importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 最重要的特征:\")\n",
    "# print(importance_df.head(10))\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(12, 6))\n",
    "# plt.barh(importance_df['feature'][:10], importance_df['coefficient'][:10])\n",
    "plt.xlabel('系数值')\n",
    "plt.ylabel('特征')\n",
    "plt.title('逻辑回归特征系数 (Top 10)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 学生代码 =====\n",
    "# TODO: 使用 cross_val_score 进行 5 折交叉验证\n",
    "# cv_scores = None  # 替换为你的代码\n",
    "\n",
    "print(\"5 折交叉验证结果:\")\n",
    "# print(f\"每折分数: {cv_scores}\")\n",
    "# print(f\"平均分数: {cv_scores.mean():.4f}\")\n",
    "# print(f\"标准差: {cv_scores.std():.4f}\")\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(8, 5))\n",
    "# plt.bar(range(1, 6), cv_scores, alpha=0.7, edgecolor='black')\n",
    "# plt.axhline(y=cv_scores.mean(), color='red', linestyle='--',\n",
    "#             label=f'平均分: {cv_scores.mean():.4f}')\n",
    "plt.xlabel('折数')\n",
    "plt.ylabel('准确率')\n",
    "plt.title('5 折交叉验证结果')\n",
    "plt.xticks(range(1, 6))\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "恭喜你完成了逻辑回归实验！在本实验中，你学习了：\n",
    "\n",
    "1. ✅ **逻辑回归原理**: Sigmoid 函数、二分类交叉熵\n",
    "2. ✅ **手动实现**: 从零实现逻辑回归梯度下降\n",
    "3. ✅ **scikit-learn**: 使用 LogisticRegression 类\n",
    "4. ✅ **模型评估**: 准确率、混淆矩阵、ROC 曲线\n",
    "5. ✅ **正则化**: L2 正则化防止过拟合\n",
    "6. ✅ **特征分析**: 特征重要性分析\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "- **Sigmoid 函数**: 将线性组合映射到 0-1 之间的概率\n",
    "- **交叉熵损失**: 衡量预测概率与真实标签之间的差距\n",
    "- **梯度下降**: 通过迭代更新参数最小化损失\n",
    "- **ROC 曲线**: 评估分类器在不同阈值下的性能\n",
    "- **特征系数**: 正系数增加恶性概率，负系数增加良性概率\n",
    "\n",
    "### 进一步学习\n",
    "\n",
    "- 尝试不同的分类阈值（不默认使用 0.5）\n",
    "- 学习逻辑回归在多分类中的应用\n",
    "- 探索其他正则化方法（L1 正则化）\n",
    "- 学习概率校准技术"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
